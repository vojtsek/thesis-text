\chapter{Experiments}
\section{Used Datasets}
\label{data-desc}
We are interested in datasets that would contain a lot of Out-of-vocabulary words.
This is desirable for us, because it means the the dataset contains a lot of words, that are not present in a vocabulary of the speech recognition and text-to-speech (TTS) that we have used.
When it comes to pronunciation of such words, text-to-speech usually performs poorly.
They should be difficult to pronounce for the TTS engine and thus there is space for an improvement.
We created two datasets ourselves, we call them $artificial$ in the following text.
\subsection{Dataset \textbf{$D_1$} - artificial}
Dataset $D_1$ contains 100 randomly chosen Dutch Names.
We take the ortographic transcription of these names and create two phonetic transcriptions.
The first is obtained from our $g2p$ module, the other is created with the procedure described in \ref{combine}.
Both transcriptions were then synthesized using the $Cereproc$ engine and rated by judgers using the CCR method introduced in \ref{tts-eval}.
\subsection{Dataset \textbf{$D_2$} - artificial}
$D_2$ is also manually labeled set of 110 words.
70 words are English, remaining 40 are Czech.
Czech words were chosen to contain lot of diacritic hence to be difficult to pronounce for TTS trained on English.
Each word was synthesized by four TTS engines, namely $Cereproc$, $MaryTTS$, $gTTS$ and $Pico$.
Each recording was then manually annotated.
\paragraph{Labeling}. Labeling of $D_2$  was performed by three annotators. Each of them had to listen every word four times, each time synthesized by different engine. He then labeled each of the recording with a discrete number ranging from one to five. The recordings labels were averaged. Thus we obtain labels per each recording and we have four labels in total for each word. We average these four labels and obtain a score between one and five, that describes overall quality of the word's pronunciation.
We thus obtain data that are suitable for our purpose, meaning, that we can explore methods of the label prediction and find out, if they correspond to the raters' opinion.
\subsection{Dataset \textbf{$D_3$} - Autonomata Spoken Name Corpus}
The Autonomata Spoken Names Corpus \cite{van2008autonomata} is a database of approximately 5.000 read-aloud first names, surnames, street names, city names and control words.
It was  published by Dutch-Flemish HLT Agency\footnote{http://tst-centrale.org/nl/}.
The corpus consists of a Dutch part and a Flemish part.
Besides the speech data, the corpus also comprises phonetic transcriptions, speaker information and lists containing the orthography of the read names.
Spoken utterances of 240 speakers living in the Netherlands (NL) or in Flanders (FL) are included in the corpus.
Since we have both ortographic and phonetic transcriptions available in addition to the audio recording, the dataset is ideal for our purposes, because we can evaluate the quality of our transcriptions.
\subsection{Librispeech}
The Librispeech dataset\footnote{http://www.openslr.org/12/} was used to train our model for Automatic Speech Recognition.
It is a corpus of approximately 1000 hours of read English speech.
\subsection{Timit}
The Timit dataset\footnote{https://catalog.ldc.upenn.edu/ldc93s1} is also a dataset for speech recognition. It is rather small containing 6300 spoken sentences, making 5.4 hours of audio in total.
All sentences were manually segmented at the phone level. 
\subsection{TODO: ptien}
\subsection{Carnegie Mellon University Dictionary}
The CMU dictionary is a pronouncing dictionary constisting of 134 000 words and their phonetic transcriptions.
Its phoneme set is based on the Arpabet\footnote{https://en.wikipedia.org/wiki/Arpabet} symbol set developed for speech recognition uses.
\subsection{Annotation procedure}
We have collected some datasets ($D_1$, $D_2$) ourselves and we had to obtain ratings for it.
Hence, we asked three annotators to label the recordings.
They were given instructions to judge the recordings with respect to quality of the pronunciation, not the sound quality of the recording .
However, the criteria were defined quite vaguely.
The raters were not told which aspects of pronunciation they should focus on.
Therefore, the ratings may be subjective according to respective annotators' preferences.
Because of that, we have explored how individual annotators' ratings correlate.
We sum up the results for the $D_1$ dataset in TODO and for the $D_2$ in Table \ref{annotatorstbl}.
\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
 \textbf{Annotators} & \textbf{Rec. 1} & \textbf{Rec. 2} & \textbf{Rec. 3} & \textbf{Rec. 4} & \textbf{mean} \\ \hline
\textbf{$A_1$ vs $A_2$} & 0.85 & 0.77 & 0.85 & 0.63 & \textbf{0.78} \\ \hline
\textbf{$A_1$ vs $A_3$} & 0.84 & 0.82 & 0.83 & 0.67 & \textbf{0.79} \\ \hline 
\textbf{$A_2$ vs $A_3$} & 0.92 & 0.89 & 0.93 & 0.88 & \textbf{0.91} \\ \hline 
\textbf{mean} & \textbf{0.87} & \textbf{0.83} & \textbf{0.87} & \textbf{0.73} &  \\ \hline 
 \end{tabular}
\end{center}
\label{annotatorstbl}
\caption{The table contains correlations that were observed among the raters.}
\end{table}

\section{Identifying difficult words}
\label{ident-diff}
If we want to improve pronunciation of the synthesized recordings, we should first identify words that are mispronounced. Obviously, we could let the TTS system pronounce each word in a best effort style and  the user would identify mistakes and correct them. This approach has several drawbacks. First, the communication with the user is rather complicated, since the incorrectly pronounced word has to be isolated prior to obtaining the correct pronunciation.
Second, it may be unpleasant for the user if he or she has to undergo this process and hear the incorrect pronunciation. Thus, it would be better if we could recognize the possibly difficult words somehow. It would mean that we can ask the user directly for the correct pronunciation of the word. We explore this issue in the rest of this section.
\subsection{Measuring the difficulty}
In order to estimate the difficulty of each word's pronunciation, we propose three measures in this section. The key idea is that we obtain values for each of this measures and then combine them in one feature vector that represents the recording.
Then we can train a classifier\ that learns to predict quality of the pronunciation.
\subsubsection{$M_1$ measure - averaged Mel Cepstral Distortion}
We discuss Mel Cepstral Distortion in detail in \ref{MCD-desc}. We use it here to define the $M_1$ measure. We can describe it as follows:
\begin{equation}
M_1(k) = \frac{1}{N}\sum_{(i,j)}{MCD(r_{ki},r_{kj})}
\end{equation}
Where $N = {3\choose2}$, $(i,j)$ stands for every combination of synthesizers, $r_{ki}$ is name {k} synthesized by engine {i} and $MCD$ is the Mel Cepstral Distortion.
The key idea motivating this measure is that if there is a problem with a pronunciation of some part of the word, every synthesizer has to deal with it somehow. It is likely, that each of them will do it in sligthly different way. This implies that the pairwise $MCD$ will increase.
\par
Nevertheless, the process of computing the value of $M_1$ introduces some problems. The troubles stem from the fact, that there is a high variability in the data obtained from the synthesizers.
That is, one synthesizer's output may differ greatly from the others, presumably if there is a big flaw in this synthesizer's pronunciation.
This influence the resulting measure heavily.
We can see the effect in \figref{m1variation}.
A solution to this problem may be, that we detect the outlying values and get rid of them.
However, it is unclear, whether this is always desired.
It can be the case, that the big difference between synthesizers' outputs occurs because the pronunciation is really difficult.
\img{m1barchart.png}{1.0}{An example demonstrating, how the measured $M_1$ between two recordings can vary. Red and blue bars represent respective recordings, their heights correspond to the measured $M_1$. On the left, there are measurings using only two synthesizers ($\sim MCD $), in the betwwen, there are triples and the most right column corresponds to four synthesizers. One outlying example can negatively influence the relevancy of the measure. While the recording represented by the red columns has quite similar MCD between every pair, the other one has outliers and thus the resulting value has got low confidence.}{m1variation}
\par
We tested, how good is the correlation of $M_1$ measure and human judgment. First, we synthesized the words by different synthesizers. Each recording created this way was then labeled by a human. Set of recordings for each word was then used to compute $M_1$ value and this was compared with the mean of the respective human ratings. Results on our sample dataset with the use of the $0$-th coefficient and without it are shown in \figref{mcdcorr}. Based on these results, it may seem that the $M_1$ measure is not very good, the best correlation was $0.471$. However we further combine it with other measures.
In section \ref{mcomb} we show, that it adds a useful piece of information to the feature vector.
\par
\img{mcd_correlation.png}{0.9}{Plot of the $M_1$ measure correlation with human judgments. Points are horizontally spaced and colored according to gold labels. The vertical axis shows $M_1$ value. The blue lines represents estimate yielded by least square fit. The top figure includes the 0-th mel cepstral coefficient, while the figure on the bottom does not.}{mcdcorr}
The $MCD$ has one property that can be looked at as a disadvantage: It does not weight its coefficients, more accurately it weights all with the same weights. We propose to change this - for example a linear model could be trained, that finds the combination of MFCC's that corresponds the best. The coefficients derived by this model would be difficult to interpret without further research. However, we can train it and try to use it instead of typical $MCD$.
\par
We want to supply the computation of $M_1$ by a linear model prediction. To train such a model, we must first prepare the data. We use the~$D_1$ dataset described in section \ref{data-desc}.
\img{mcdexample.png}{0.9}{The process of aggregating the values obtained from mel cepstral analysis. Each recording is represented by sequence of $M$-sized vectors. These sequences are summed, thus the representation is flattened into one vector of length $M$.}{mcdexample}
\par
Since we want to use a simple linear model, we need to work with fixed size vectors. Nevertheless, the input data are variable-length sequences of vectors. We first transform each synthesized recording by summing all its mel cepstral coefficients. Thus we get ~$N$~$M$-sized vectors, where~$N$ is the number of synthesizers used and $M$ is the order of mel cepstral analysis. This step is visualized in \figref{mcdexample}. We then compute differences in each position between every pair of recognizers and sum those to obtain one resulting vector of length~$M$. We use labels obtained from annotators normalized to interval $[0, 1]$ as a target variable. We then evaluate the model using 5-fold cross-validation. The results are shown in \figref{cvfolds}. The correlations and~$R^2$ for respective folds are shown in Table \ref{cv}.
\img{cv_folds.png}{0.9}{Plots of the correlation of the measure computed by model in cross-validation folds. The predicted values are on the vertical axis, the horizontal position represents the human judgments. The blue lines shows least-squares fit. Although the training data are rather small, it shows, that the model can output meaningful values.}{cvfolds}
\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 \textbf{Fold} & \textbf{Correlation} & $\boldsymbol{R^2}$ \\ \hline
 1 & 0.77 & 0.74 \\ \hline
 2 & 0.03 & 0.75 \\ \hline
 3 & 0.82 & 0.76 \\ \hline
 4 & 0.35 & 0.77 \\ \hline
 5 & 0.69 & 0.74 \\ \hline
 6 & 0.53 & 0.72 \\ \hline
 7 & 0.94 & 0.75 \\ \hline
 8 & 0.68 & 0.73 \\ \hline
 \textbf{Mean} & \textbf{0.60} & \textbf{0.75} \\ \hline
 
 \end{tabular}
\end{center}
\label{cv}
\caption{The table contains results of linear regression model training on aggregated MFCC vectors. Correlation with human judgment and $R^2$ measure for respective cross-validation folds are shown.}
\end{table}
\subsubsection{$M_2$ measure - averaged phonetic distance}
Another measure we can possibly use is based on phonetic transcriptions. We first recognize the recordings with a phoneme recognizer, thus we obtain a sequence of characters per each recording. We can compute pairwise distances of these transcriptions, using Levenshtein \cite{navarro2001guided} distance which we normalize by the length of the word. The motivation is similar to the $M_1$ measure. Assuming the difficult words have positions that are problematic for the TTS engine, the recognized phonemes on these positions should differ and thus the distance between transcriptions should increase. Four our purposes, we have used Levenshtein distance as a metric.
\par
The M2 measure is described by equation:
\begin{equation}
M_2(k) = \frac{1}{N}\sum_{(i,j)}{\frac{LD(Hyp(r_{ki}),Hyp(r_{kj}))}{max(len(r_{ki},len(r_{kj})))}}
\end{equation}
Where $N = {3\choose2}$, $(i,j)$ stands for every combination of synthesizers and $r_{ki}$ is name $\{k\}$ synthesized by engine $\{i\}$. $LD$ means Levenshtein Distance and $Hyp$ represents the best hypothesis from the phonetic recognizer. Note, that we normalize the distance by length of the longer transcription.
\img{mcdphcorr.jpg}{0.9}{Scatterplot showing relation between the $M_1$ and $M_2$ measures. We can see, that the measures are rather uncorrelated. However, they can still be complementary.}{mcdphcorr}

\subsubsection{$M_3$ measure - occurrence of confusable bigrams}
The $M_3$ measure is based on a different approach.
We want to learn the typical mistakes of a grapheme-to-phoneme converter, more specificaly we want to explore which groups of graphemes are difficult to transcribe for the $g2p$.
We suppose that words with many occurences of such groups are difficult to pronounce.
To compute the measure, we need two corpuses ($C_1$ and $C_2$) in different languages and a $grapheme-to-phoneme\:(g2p)$ training algorithm.
The corpuses should consist of pairs (word, transcription).
We can then prepare for computation of $M_3$ in three stages:
\begin{enumerate}
\item Train $g2p$ model $G_1$ on corpus $C_1$ in language $L_1$.
\item Use trained $G_1$ to transcribe words contained in corpus $C_2$.
\item Identify problematic parts.
\end{enumerate}
Stage 3 needs further description.
\par
We obtained list of triples, $i^{th}$ of which is structured:
\begin{center}
(word $w_i$ from $C_2$, original transcription $t^o_i$, transcription $t^h_i$ derived by $G_1$)
\end{center}
We can then use the Dynamic Time Warping algorithm to obtain pairwise alignments of these sequences, illustrated on \figref{sample_dtw_alignment}
In fact, we first transform the phoneme sequences into sequences of bigrams.
This is because graphemes and phonemes are not in one-to-one correspondence and bigrams capture the relations better.
\img{sample_align.jpg}{0.35}{Sample alignment of two phonetic sequences}{sample_dtw_alignment}
Once we have the alignments, we can identify positions, in which the original transcriptions from $C_2$ differ from the hypothesis derived in Stage 2.
Respective bigrams from the original word can then be identified.
For each bigram, we count number of times it was marked as difficult.
Each word can then be scored according to number of bad bigrams it contains.
\par
To evaluate the $M_3$ measure, we have to synthesize the words, have them rated and compare the ratings with the measure.
MaryTTS framework was used, because we can extract phonetic transcriptions from it and thus use its $g2p$ module.
This means that we are able to derive the transcriptions with exactly the same module that is used in the synthesizer.
We have also used the Cereproc engine.
The words were chosen as a subset of the Autonomata Corpus.
\par
Because we do not have access to the $g2p$ that Cereproc uses, we trained our own model on the CMU dictionary\footnote{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}.
We use a subsets of Autonomata corpus as the dataset of foreign worlds.
The data was then processed and the confusion matrix was created, containing number of times, respective bigrams are confused with each other.
The matrix turns out to be quite sparse, which is not surprising, since the majority of bigram pairs are interchanged with probability nearly zero.
\par
If we restrict the matrix only to values greater than a certain threshold, its dimensions decrease dramatically and we can visualize it; the result can be seen in \figref{bigram_conf}
The scoring procedure counts for each word a number of occurrences of bigrams that were confused and the number of confusions is summed and divided by the length of the word.
We plot the scores in \figref{bigram_scores}.
The shape of the curve corresponds to the fact that many bigrams do not occur a lot, or are not problematic.
\par
The correlation of this measure and annotations was $0.20$ for the MaryTTS and $0.31$ for the Cereproc, respectively.
Intuitively, if we have access to the $g2p$ module, which is the MaryTTS case, it is expectable, that the results would be better.
However, this turns out not to be true.
It may be caused by the fact, that the recordings created by Cereproc are of better quality in general, so the ratings are better as well.
\img{bigroccurrs.jpg}{0.9}{Frequencies of confusing respective bigrams, i.e. how many times it occurs in some confused pair. We can see, that the majority of bigrams has very low frequencies.}{bigram_scores}
\img{confusion_limit.png}{0.9}{Visualization of the confusion matrix computed from the results.}{bigram_conf}
\subsection{Measure combination}
\label{mcomb}
We have seen that neither of the measures correlates well with the annotators' labeling (\figref{mcdcorr}).
Despite this fact, it may by interesting to explore, whether the combination of the measures can work better.
As described in the introduction to this section, we combine the measured values to one feature vector that represents the word.
However, it does not make much sense to include the $M_3$ measure, since it is language-dependent.
Namely, it captures only the specific differences between English and Dutch.
Since our dataset contains OOV words from Czech, we do not include $M_3$ to the feature vector.
We use a \textit{Ridge regression}~\cite{hoerl1970ridge} model, which is a regularized version of the linear regression.
It adds to the objective function quadratic norm of the weight vector.
We train the model using averaged human labels as a target variable.
We have used 5 fold cross validation and measured the $R^2$ and $Mean Squared Error (MSE)$, which we averaged over all the folds.
The measured $MSE$ was $0.39$ and $R^2$ was $0.58$.
We remind, that the human labels are discrete values in the interval $[1,5]$, so the $MSE$ value is good, saying that we differ from the human label by $0.6$ in average.
We plot the differences from true labels in \figref{ident-box}.
\img{identify-predictions-boxplot.png}{0.9}{A boxplot showing difference of the predicted labels from human ratings. We can see, that majority of predictions differ by 0.8 at maximum.}{ident-box}
\par
We are interested not as much in the label prediction, as in the identification of words that are difficult to pronounce.
To achieve this, we can choose a threshold and claim that if the model's predicted value is greater than the threshold, the respective word is hard to pronounce and we should try to improve the pronunciation.
We could empirically determine a threshold separating difficult words from the easier ones.
If we plot the results, as in \figref{meas-combined}, we can see, that setting the threshold to value 2.5, which is exactly in the middle of the scale, is reasonable choice.
Thus we can reformulate the problem as a binary classification task which gives us another insight.
Simply, we have to decide if a given example will be pronounced correctly or not.
We aim to identify incorrectly pronounced words and we refer to these words as positive examples.
\par
Thus we can compute the \textit{precision and recall} and obtain the \textit{$F_1$ score} 0.86.
Based on these experiments, our method has reasonable results and may be used to identify difficult recordings confidently.
\img{identify-predictions.png}{1.0}{Plot of the labels given by humans (vertical axis) and predicted values (horizontal axis). The vertical line represents the threshold with respect to the predictions, the horizontal dashed line ilustrates true division.}{meas-combined}
\par
Since we have approached the problem as a binary classification task, we can also train a classifier.
We train a Logistic Regression classifier and determine a classification threshold by plotting the Receiver Operating Characteristics line - \figref{roc}.
Since we want to identify the bad transcriptions, we focus on $Recall$ more.
We choose threshold as plotted and obtain \textit{$F_1$ score} 0.85.
This is slightly worse the the \textit{$F_1$} obtained from the linear regression model.
Based on this fact, the linear regression model is at least comparable to the binary classifier in the classification task.
In addition, it gives us additional information, that is a quantification of pronunciation difficulty.
\img{roc.png}{0.9}{A Receiver Operating Characteristics line of Logistic Regression classifier. The green dashed line indicates a random choice, the red point corresponds to the chosen threshold.}{roc}
\subsection{Discussion}
We proposed and explored three measures for a purpose of recognition of words that are difficult to pronounce for a TTS system.
Our goal is to develop a method that would identify such words before they are introduced to the user.
Exploration of these measures showed, that they does not correlate well with the human judgments.
Nevertheless, the experiments indicate, that the measures are complementary in the sense, that if we combine them in a feature vector, we can train a model that predicts reasonable values.
If we determine a threshold, we can use this model for classification purposes.
Although the results are promising, the computation process that precedes the creation of the feature vector is not very convenient.
We have to have access to speech engines, non-trivial phonetic corpuses ($M_3$) and phoneme recognizer ($M_2$).
We also have trained a model that substitutes the MCD computation.
It yields meaningful results, however the disadvantage of this approach is that we need labeled data, while the previous method works in an unsupervised way.
\par
As for the $M_3$, its disadvantage is that it relies quite heavily on the alignment process, which is imperfect.
Because og that, it can sometimes mark as confused pair of bigrams that is not correctly aligned.
Also id occurrences of some bigrams are very sparse, then its scores may be estimated poorly.
Another possible problem is, that it relies on the corpuses with phonetic transcriptions and then it is specific to certain pair of languages.
It did not prove to be very useful.
\section{Improving the pronunciation}
\label{pron-improvement}
\subsection{Overview}
We now briefly remind the task we consider.
It may occur, that the TTS system pronounce certain words incorrectly.
This is caused by the fact, that these words are not present in the vocabulary, that is, we do not have access to respective phonetic transcriptions.
Thus we need to derive these transcriptions somehow.
We can use a $g2p$ module which we introduce in section \ref{g2p-desc}.
However as we discuss there, the $g2p$ is not able to handle pronunciations of groups of graphemes that should be pronounced differently than in the language the TTS is trained on.
Also, we do not have access to information, which language we should use to get the correct pronunciations.
Therefore, the use of $g2p$ is not possible or at least very problematic.
We have also discussed methods (section \ref{pronunc-spoken}), that derive pronunciation from spoken examples directly.
However, such methods usually require quite substantial modifications to the speech recognizer.
\subsection{Exploiting the speech recgonizer}
We try to exploit information obtained from the speech recognizer to improve the pronunciations.
The very first step in the derivation process is to use the $g2p$ module and obtain baseline phonetic transcriptions from it.
Next, we need to get the phonetic transcription.
Hence, we feed the spoken version of the utterance into a speech recognizer and decode output as we describe in section \ref{ph-rec}.
In \ref{nbest-detail} we notice, that an interesting piece of information is contained in the  recognizer's $n$-best list.
We propose to exploit the information by clustering hypotheses in $n$-best list to a fixed number of clusters.
Thus we obtain more compact yet sufficiently variable version of it.
We then want to use this compressed list to choose the best transcription as we discuss in detail later.
\subsubsection{Getting the recognizer}
First of all, we have to get our speech recognizer.
We use Kaldi \footnote{http://kaldi-asr.org/} speech recognition toolkit for the model training and subsequent decoding.
In section \ref{asr-decoding} we describe, how we can modify the deocding graph to output phonemes.
Thus we are not limited by the choice of acoustic model, we simply do the necessary modifications to the decoding graph.
\par
We have explored, how well different methods perform.
In \tabref{PERtable} we provide short summary of the results.
\figref{decoderPER} shows dependency of the Oracle Phone Error Rate on the depth of the nbest list.
The problem with evaluation is, that the reference must contain phonetic transcritptions.
We have used our $g2p$ module to obtain them.
Thus our transcriptions are not ground truth and our evaluation is biased.
However, the conditions are equal for each setting so it can help us to choose.
We now present shortly the mentioned methods, details can be found in section \ref{ph-rec}:
\begin{itemize}
\item $DecoderVocab$ was created by changing the vocabulary that is used for decoding. Thus, there does not exist a mapping from phoneme sequences to words as usual, each phoneme is mapped to itself instead. The acoustics were modeled using Gaussian Mixture Models\footnote{http://kaldi-asr.org/doc/model.html}.
\item $DecoderBi$ refers to decoder created to model bigrams of phonemes, that is, it includes contextual information. The acoustics were modeled using Gaussian Mixture Models\footnote{http://kaldi-asr.org/doc/model.html}.
\item $DecoderBi\:-\:chain$ has same type of decoding graph, but the acoustic part of the decoder was obtained from the Kaldi chain model\footnote{http://kaldi-asr.org/doc/chain.html}.
\item $Timit$ decoder was trained on the Timit dataset\footnote{https://catalog.ldc.upenn.edu/ldc93s1} directly. This dataset has a vocabulary that contains phonemes, its language model is bulit on phonemes too.
That menas the decoder outputs phonetic representantion of words.
\end{itemize}
For training, a subset of Librispeech corpus\footnote{http://www.openslr.org/12/} or Timit{https://catalog.ldc.upenn.edu/ldc93s1} were used. The evaluation was done on the TODO:ptien-ny.
Based on the results, we choose the decoder acoustic part of which is modelled by the chain model and the decoding graph models phones as bigrams.
\begin{table}
\centering
\begin{tabular}{ |c|c|c|c| } 
 \hline
 Decoder type & Phoneme Error Rate & Oracle PER (3best) & Oracle PER (5best) \\ 
 \hline
 DecoderVocab & 90.1 & - & - \\ 
 \hline
 DecoderBi & 66.3 & - & - \\
 \hline
 DecoderBi - chain & \textbf{61.47} & \textbf{60.20} & \textbf{59.69} \\
 \hline
 Timit & 72.1 & - & - \\
 \hline
\end{tabular}
\label{PERtable}
\caption{Overview of results in terms of Phone Error Rate (PER) or Oracle PER, respectively. We measured the Oracle PER only on the most succesful decoder. The best results are showed in bold.}
\end{table}
\img{decoderPER.png}{0.9}{Dependency of decoder's Oracle Phone Error Rate on the depth of the $n$-best list we explore. The bigger part of the list is exploited, the lower is the error rate.}{decoderPER}
\subsubsection{Compressing the $n$-best list}
\label{nbest-clust}
To cluster the items, we first need to choose the clustering method.
We explore two methods here, namely \textit{kmeans} and \textit{spectral clustering}.
We now briefly dicsuss pros and cons of each.
\begin{itemize}
\item \textit{Kmeans clustering} works with vectors in euclidean space and thus understandably uses \textit{euclidean distance}.
We thus transform the hypotheses into $count\: vectors$.
$Count\:vector$ is a vector that represents strings as eucliden vectors.
Each position in it represents one character from the alphabet and the number on that position is the number of occurences of this character in the string.
This aprroach does not capture the context of the occurences, but i can be extended to capture the occurences of $bigrams$ (i.e. character pairs) in the same way.
\item In case of \textit{spectral clustering}, we have to decide, which distance metric we will use.
Since the hypotheses that we want to work with are sequences of phonemes (i.e. strings), either \textit{Hamming}~\footnote{https://en.wikipedia.org/wiki/Hamming\_distance} or \textit{Levenshtein\_distance}~\footnote{https://en.wikipedia.org/wiki/Levenshtein\_distance} come into consideration.
However, the \textit{Hamming} distance is not usable, because it counts number of different positions in two strings of the same length.
Unfortunately, this is not the case of the hypotheses gathered from the decoder.
Thus we use \textit{Levenshtein} distance to be our metric.
\end{itemize}
\par
For ilustration, we give examples of the resulting clustering in \figref{cluster-spect} or \figref{cluster-kmeans}, respectively.
It is difficult to decide, which method is more appropriate.
Based on results and observations, we choose the \textit{kmeans clustering} to be our clustering method.
The clusters derived by \textit{kmeans} appears to be more similair and coherent, especially if we use longer list of hypotheses.
We do not show longer lists here for readability purposes.
\img{spectral-clusters.png}{0.9}{An example of clustering hypotheses of the phone recognizer with spectral clustering algorithm. The input was spoken czech word "\v{C}okol\'{a}da".}{cluster-spect}
\img{kmeans-clusters.png}{0.9}{An example of clustering hypotheses of the phone recognizer with kmeans clustering algorithm. The input was spoken czech word "\v{C}okol\'{a}da".}{cluster-kmeans}
\subsubsection{Choosing the best transcription}
\label{choosing-best}
We are now in the situation that we have a fixed-length list of hypotheses and we want to choose the most apropriate one.
Therefore, we formulated the task as a problem of classification.
How can we determine the best hypothesis?
We want to choose one that is the closest to the reference phonetic transcription in terms of \textit{Levenshtein} distance.
\par
We want to approach the problem as a machine learning task - train a model, that classifies a list of $N$ items (hyptoheses) to class $1\dots N$, provided that the list belongs to class $k$ if the hypothesis on the $k-th$ position is the best.
This is just different way of saying, that we want to choose the best hypothesis among others.
Nevertheless, the list of hypotheses alone does not provide sufficient information for the classifier.
Hence, we base our decision not only on the list of hypotheses but also on the ortographic form of the word.
\par
In this experiment, we fix the length of the list to five.
The data for this experiment comes from the $D_3$ dataset.
Each spoken name was recognized with our recognizer, 20-best list was clusterized and 5 representatives were chosen.
In order to explore the model behavior we actually created two datasets.
The first one was created according to the procedure we have described previously.
In the second one however, the reference phonetic transcription has been mixed among the hypotheses, replacing one of the alternatives.
The latter setting should be much easier to deal with, because the reference transcription is different from the rest.
Ideally, the reference should be chosen every time.
We include the second version of the dataset to find out, if the model has an ability to choose correctly using our proposed input.
\par
The sequences can vary in length, but the models need the inputs to be of fixed length.
Thus we have to encode the data somehow to achieve this.
One option is to use the Bag of $N$-grams (BoN) technique.
BoN is similar to Bag of Words\footnote{https://en.wikipedia.org/wiki/Bag-of-words\_model}.
It is used to transform arbritrary-lengtth finite strings coming from a finite alphabet into fixed length vectors.
First, the number of occurrences of respective $n$-grams in the sequence is counted.
Since the alphabet is finite, the number of $n$-grams is limited and we can construct an integer vector and assign each $n$-gram fixed position in it (i.e. enumerate the $n$-grams).
The desired semantics is, that the number on the $i^{th}$ position expresses the number of occurences of the $i^{th}$ $n$-gram in the string.
We provide example of the decoding in \figref{bon}.
\par
In general, the $n$ can be chosen arbitrarily, nevertheless, the length of the vector increases exponentially with respect to $n$, because number of $n$-grams over an alphabet od size $m$ is $m^n$.
High dimensional vectors are not very suitable for most of machine learning techniques because of the data sparsity and many model parameters.
Thus we choose $n \le 3$.
In our experiments we include all shorter n-grams in the feature vector, i.e. if we choose $n=3$ we track not only number of trigrams but also bigrams and unigrams.
\img{bon-encoding.png}{0.9}{An example of encoding word \textit{"moon"} using BoN technique with $n\:=\:1$. The reduced Latin alphabet was used.}{bon}
\par
After transformation to the fixed-length vectors, the encoded hypotheses are shuffled and concatenated, so a feature vector is formed.
The items are shuffled to prevent the model learning to give meaning to the ordering.
Rather it should learn to choose the best transcription according to its relationship with other transcriptions and the reference.
Supposing that we have used $l$ hypotheses, the feature vector looks like:
$$h_{\pi(1)}.h_{\pi(2)}.\dots.h_{\pi(l)}$$
Where $\pi: \{1\dots l\} \rightarrow \{1\dots l\}$ is a random permutation and the dots between hypotheses represents a concatenation operation, no separator characters are used.
Thus, if the size of alphabet is $K$, we choose a maximal $n$-gram and use $l$ hypotheses, the length of each feature vector is $L = (K + K^2 + \dots + K^n) \times (l+1)$.
\par
Next, we have to add target labels to the feature vectors.
Levenshtein distance of each hypothesis and reference phonetic transcription is computed and the hypothesis with the smallest distance is marked as the best.
The index of this hypothesis is then used as a target label.
It is correct to use a reference transcription in the data preparation phase to derive a label, it does not mean that we use it in the training phase.
\par
To solve the classification task, machine learning theory gives us a large portfolio of methods.
We try several classifiers, namely Support Vector Machines, Logistic Regression, Random Forest and Multi Layer Perceptron.
In the experiment settings, we have used hypotheses lists of length 4.
Datasets of different sizes were used, all created from a subset of the $D_3$ dataset.
We can see the dependency of accuracy on the dataset size in the \figref{size-acc}.
The data were split to training, validation and test sets in ratio $7:2:1$.
Overview of the respective accuracies is given in Table \ref{tabacc}.
\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 \textbf{Classifier} & \textbf{Acc. with gold} & \textbf{Acc. no gold} \\ \hline
 Most Frequent Class & 0.26 & 0.43 \\ \hline 
 Linear regression & \textbf{0.987} & \textbf{0.53} \\ \hline
 Random Forest & 0.99 & \textcolor{red}{0.40}\\ \hline
 SVC linear & 0.98 & 0.48 \\ \hline
 SVC rbf & 0.97 & \textcolor{red}{0.41} \\ \hline
 SVC polynomial & 0.26 & \textcolor{red}{0.41} \\ \hline
 Multi Layer Perceptron & \textbf{0.99} & 0.45 \\ \hline
 \end{tabular}
\end{center}
\label{tabacc}
\caption{Test set accuracies of various models both with and without the reference transcription included. The baseline is set by trivial classifier, which assigns each example the most frequent label. Red values indicates, that the model does not outperform the baseline, bold values are the beast achieved results.}
\end{table}
\img{size_acc.png}{0.9}{The dependency of the Linear Regression model accuracy on the size of the data it is trained on. The red dashed line represents the performance of trivial classifier. We can see, that with sufficient data, the model outperforms the baseline.}{size-acc}
\paragraph{Discussion}
We can see, that the classifiers perform much better when the reference transcription is included in the data.
This can be caused by the fact, tha the reference transcription should be quite different from the rest, so it is supposed to be easily recognizable.
Also, the distribution of classes is nearly uniform when the reference is included.
This is in contrast with the real settings, because there the distribution is biased, even after shuffling as we can see in \figref{best-distr}.
\par
Some of the models used did not even outperform the trivial classifer that chooses always the most frequent class.
Nevertheless, some of them did, namely the Logistic Regression, Multi Layer Perceptron and Support Vector Classifier with linear kernel.
Therefore it makes sense to include the process of selection of the best hypotheses from the $n$-best list.
\img{distribution.png}{0.9}{Distribution of target classes in the dataset. Although the hypotheses lists have been shuffled, the distribution is biased, making the task more difficult.}{best-distr}
\subsection{Combining acoustic and textual information}
\label{combine}
In the previous, we have managed to derive phonetic transcriptions and pick the most appropriate one.
Problem is that even the best transcriptions are too different from the desired ones.
They cannot be used directly as input for the TTS.
However, they capture pronunciations of some of the desired parts and thus can bring in some essential information.
On the other hand, $g2p$'s output, is close to the correct transcription, just with some mistakes.
As we discussed in section \ref{g2p-desc}, it is difficult to correct these mistakes only with $g2p$, but there is a possibility to employ the phonetic transcriptions.
The idea is to identify positions, where the $g2p$ is unsure and replace these with the respective counterparts in the speech recognizer's output.
\par
The question also arises, how to evaluate the acquired transcriptions once we have them.
In section \ref{tts-eval} we introduce the A-B testing.
This is suitable for our purpose, however, we first have to choose, to what we want to compare.
Clearly we cannot compare to the original transcription, since it is supposed to be the correct one.
The baseline is actually set by the $g2p$ output - we want to outperform it.
Therefore we synthesize transcriptions from the $g2p$ as well as the improved ones. And we let the raters choose which one is better.
\par
One option how to identify such positions is to let the $g2p$ give more alternatives.
We then align those alternatives using Dynamic Time Warping Algorithm and pick positions, where many alternatives are possible.
We assume, that confidence of such positions is low.
Then we align the $g2p$ outputs with the ASR transcription and do the replacement on respective positions.
\par
We propose an alternative to the previous approach which is more exact.
After some modifications to the $g2p$ module, we can make it output posterior probabilities of the individual characters.
Similarly, we can obtaind posterior probabilities from the ASR decoder's output.
That means, if we have transcription $\mathbf{W}$ composed of phonemes:
$\mathbf{W} = w_1\dots w_n$, we obtain probabilities $\mathbf{P}(w_i \vert w_1 \dots w_{i-1})$.
We can obtain an $n$-best lists from both the decoder and the $g2p$ module.
Thus we can combine the information and create Finite State Transducer the represents all the possilbe paths as ilustrated in the \figref{fst-combined}.
\par
The idea is, that if the $g2p$ has low confidence at some poisition $i$, we replace the position by the respective part of decoder's output, if it is more confident here.
however, the problem is, that the transcriptions may be of different length.
Thus we have to employ Dynamic Time Warping (DTW) algorithm again to align the transcriptions.
\img{fst-combined.png}{0.55}{An ilustrative example of the FST combined from the $g2p$ (top) and speech decoder (bottom). A part of input is chosen corresponding to the syllable "\v{C}O" and the chosen path is highlighted. It can be seen, that the path goes through both the decoder's and $g2p$'s hypotheses.}{fst-combined}
\par
It is not straightforward how to use the DTW.
Another problem is, that although we obtained posterior probabilities, the $g2p$ and the decoder treat their graphs differently, namely the decoder assigns greater probabilities to the individual phonemes, thus restricting number of alternatives.
Therefore, we have to weight the decoder's probabilites if we want to combine the results.
We could for example try to train a model that would learn the weights.
We left this research for future work.
Nevertheless, we ran some preliminary experiments showing, that it is promising way to go, as showed in \figref{preliminary}.
\img{preliminary.png}{0.65}{An example of the preliminary result derived from combined FST. We can see, that the $g2p$ transcription is rather incorrect. We higlight groups of phonemes, where the transcription is changed so it is more similair to the original. However we can see that some other phonemes were replaced inocrrectly and the result is far from perfect yet. The transcriptions are given in IPA.}{preliminary}
\paragraph{Discussion}
We have proposed two methods of combining the acoustic and textual information.
Both methods suffer from the fact that the transcriptions are of different length and misaligned.
The use of Dynamic Time Warping algorithm may not be good enough.
The resulting alignments are not always desired since it may pair two phonemes that should not be aligned.
Also, the method that combines the FST's is not exact yet, mainly we did not derive the weights for the speech decoder's hypotheses.
However, looking at the ratings and preliminary results, this is a promising way to go.