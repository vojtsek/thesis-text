\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces A high level architecture of spoken dialogue system. The data flow between respective components is showed. We can see, that different components may work with different data representations.}}{3}{figure.1.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Sample dialogue illustrating how the pronunciation correction process may look like. The dialogue system pronounces user's name incorrectly. User then correct it and the system learns. The transcriptions of the user's name are given in ARPABET.}}{4}{figure.1.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of Finite State Transducers and their composition}}{9}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Overview of a typical statistical Text-to-speech system architecture. The text is first analyzed and translated to phonetic transcription. Then, the acoustic parameters are estimated. Finally, the speech is synthesized.}}{12}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of SSML code}}{13}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of dendrogram outputted by hierarchical clustering methods.}}{15}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Some letter of the IPA alphabet together with their occurrences in English words.}}{17}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces An example set of words used to perform the Diagnostic Rhyme Test}}{18}{figure.2.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample alignment of two phonetic sequences}}{28}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces An example of the finite state transducer representing the lexicon\cite {mohri2002weighted}. Labels "a:b" represents the edge's input and output symbols. This transducer is not weighted, since it represents only the lexicon, i.e. all possibilities. The decoding process goes from left to right, transforming the matching input string to the output.}}{30}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A plot of Oracle Phone Error Rate dependency on the depth of the $n$-best list we explore. We can see, that the Oracle PER decreases with the depth, so it makes sense to try to exploit the hypotheses located deeper.}}{33}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Histogram that visualizes the distribution of the hypothesis, that is the closest to the gold reference utterance in terms of Phone Error Rate. The bars show the number of times the best hypothesis was found on the particular position. Two settings are included in this figure: the green bars describe situation, when first five positions were used, the blue ones were measured using nine positions.}}{33}{figure.3.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces An example of the synthesizer's input in SSML, forcing it to use our phonetic transcription.}}{36}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces An example demonstrating, how the measured $M_1$ between two recordings can vary. Colors (red and blue) represent recordings corresponding to two names. The bars' heights correspond to the measured $M_1$. On the left, there are measurings using only two synthesizers ($\sim MCD $), in the between, there are triples and the most right column corresponds to four synthesizers. As we can see for the word represented by blue, one outlying example can negatively influence the relevancy of the measure - some of the bars are high while some of them not. Therefore, the result has low confidence. The word represented by the red bars has quite similar $M_1$ between every pair.}}{37}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Plot of the $M_1$ measure correlation with human judgments. Points are horizontally spaced and colored according to gold labels - green points correspond to well rated recordings (average rating better then 1.75), blue are neutral (rating 1.75 - 2.5) and red are rated as bad (rating worse than 2.5). The vertical axis shows $M_1$ value. The blue lines represents estimate yielded by least square fit. The top figure includes the \textit {0-th} mel cepstral coefficient, while the figure on the bottom does not.}}{38}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The process of aggregating the values obtained from mel cepstral analysis. Each recording is represented by sequence of $M$-sized vectors. These sequences are summed, thus the representation is flattened into one vector of length $M$.}}{39}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Plots of the correlation of the measure computed by model in cross-validation folds. The predicted values are on the vertical axis, the horizontal position represents the human judgments. The blue lines shows least-squares fit. Although the training data are rather small, it shows, that the model can output meaningful values.}}{40}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Scatterplot showing relation between the $M_1$ and $M_2$ measures. Vertical axis corresponds to $M_1$, horizontal to $M_2$. We can see, that the measures are rather uncorrelated. However, they can still be complementary.}}{41}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Frequencies of confusing respective bigrams, i.e. how many times it occurs in some confused pair. We can see, that the majority of bigrams has very low frequencies.}}{42}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Visualization of the confusion matrix computed from the results. In this visualization, most of the rows and columns were ommited, because they contain a lot of small values. The lighter the position is, the bigger is the number of ocurrences. We can see, that few of the bigrams are confused.}}{43}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces A boxplot showing difference of the predicted labels from human ratings. We can see, that majority of predictions differ by 0.8 at maximum.}}{44}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Plot of the labels given by humans (vertical axis) and predicted values (horizontal axis). The vertical line represents the threshold with respect to the predictions, the horizontal dashed line ilustrates true division.}}{45}{figure.4.10}
\contentsline {figure}{\numberline {4.11}{\ignorespaces A Receiver Operating Characteristics line of Logistic Regression classifier. The green dashed line indicates a random choice, the red point corresponds to the chosen threshold.}}{46}{figure.4.11}
\contentsline {figure}{\numberline {4.12}{\ignorespaces Dependency of decoder's Oracle Phone Error Rate on the depth of the $n$-best list we explore. The bigger part of the list is exploited, the lower is the error rate.}}{47}{figure.4.12}
\contentsline {figure}{\numberline {4.13}{\ignorespaces An example of clustering hypotheses of the phone recognizer with spectral clustering algorithm. The input was spoken czech word "\v {C}okol\'{a}da".}}{48}{figure.4.13}
\contentsline {figure}{\numberline {4.14}{\ignorespaces An example of clustering hypotheses of the phone recognizer with kmeans clustering algorithm. The input was spoken czech word "\v {C}okol\'{a}da".}}{49}{figure.4.14}
\contentsline {figure}{\numberline {4.15}{\ignorespaces An example of encoding word \textit {"moon"} using BoN technique with $n\tmspace +\medmuskip {.2222em}=\tmspace +\medmuskip {.2222em}1$. The reduced Latin alphabet was used.}}{50}{figure.4.15}
\contentsline {figure}{\numberline {4.16}{\ignorespaces The dependency of the Linear Regression model accuracy on the size of the data it is trained on. The red dashed line represents the performance of trivial classifier. We can see, that with sufficient data, the model outperforms the baseline.}}{52}{figure.4.16}
\contentsline {figure}{\numberline {4.17}{\ignorespaces Distribution of target classes in the dataset. Although the hypotheses lists have been shuffled, the distribution is biased, making the task more difficult.}}{53}{figure.4.17}
\contentsline {figure}{\numberline {4.18}{\ignorespaces An ilustrative example of the FST combined from the $g2p$ (top) and speech decoder (bottom). A part of input is chosen corresponding to the syllable "\v {C}O" and the chosen path is highlighted. It can be seen, that the path goes through both the decoder's and $g2p$'s hypotheses.}}{53}{figure.4.18}
\contentsline {figure}{\numberline {4.19}{\ignorespaces An example of the preliminary result derived from combined FST. We can see, that the $g2p$ transcription is rather incorrect. We higlight groups of phonemes, where the transcription is changed so it is more similair to the original. However we can see that some other phonemes were replaced inocrrectly and the result is far from perfect yet. The transcriptions are given in IPA.}}{54}{figure.4.19}
\addvspace {10\p@ }
\addvspace {10\p@ }
