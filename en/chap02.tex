\chapter{Overview of used techniques and algorithms}
\section{Audio signal processing}
\label{ASP-desc}
Since we want to work with the speech signal, which is continuous, we have to transform it to a more appropriate representation.
The goal of this processing step is to express the audio signal as a sequence of fixed-length vectors.
We sketch here the basic principles as described in \cite{taylor2009text}, so we can understand our input data.
Speech signal in the real world are mechanical waves, so it is obviously analogous quantity.
When we process the speech signal with computers, it is assumed to be digitised, so it is converted into discrete form.
When performing digital signal processing, we are usually concerned with three key issues.
\begin{enumerate}
\item To remove the influence of phase, because the ear is not sensitive to phase information in speech, and we can use frequency domain representation.
\item Performing source/filter separation, so that we can study the \textit{spectral envelope} of sounds.
Spectral envelope characterizes the frequency spectrum of the signal, which is essential for the speech recognition and processing.
\item We often wish to transform these spectral envelopes and source signals into more efficient representations.
\end{enumerate}
To sum up, we want to convert the signal in such way, that we emphasize important aspects like frequency spectrum and yet we can easily process it.
Overview of the main standard processing steps follows.
\par
The input signal is divided into \textit{windows}.
Windowing considers only some part of the signal.
Usually, the signal is transformed at window borders to prevent discontinuities.
This is achieved for example by use of \textit{Hamming} window.
The complete waveform is therefore a series of windows, that are sometimes called \textit{frames}.
The frames overlap, this is influenced by the size of the \textit{frame shift}.
Inside one frame, we assume, that the speech signal is stationary and we transform it to a frequency domain by Discrete Fourier Transformation\footnote{https://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}.
\par
It is better for observing the natural speech characteristics, to use a logarithm scale instead of linear.
In fact, the \textit{mel scale}\footnote{https://en.wikipedia.org/wiki/Mel\_scale} is frequently used, which even better corresponds with the human perception.
Further, the so called \textit{cepstrum}\footnote{https://en.wikipedia.org/wiki/Cepstrum} is computed from the log magnitude spectrum, by performing inverse Fourier transformation.
Cepstrum can be represented by coefficients, number of which can be chosen.
Those are called the \textit{Mel Frequency Cepstral Coefficients(MFCCs)}.
To sum up, we have described how to process the digital audio signal and convert it into discrete series of overlapping frames, each of which is represented by a fixed-length vector of MFCCs.
\par
We gave a brief overview of the signal processing procedure that is essential to work with the audio signal.
When we mention the input speech signal, we mean series of MFCC vectors, unless explicitly stated otherwise.
It is not straightforward to say, whether each coefficient has its interpretation - at least it is dependent on the size of the vector we use.
Nevertheless, each coefficient is more or less related to some aspect of the speech.
\section{Mel Cepstral distortion}
\label{MCD-desc}
When considering synthesized speech, we often want to measure its quality.
In general, this is a very difficult task - different aspects are important regarding the speech quality.
Naturalness and intelligibility are often considered as main aspects.
Usually, human raters are employed to judge the quality of the speech, however some objective measure were described too.
One of them is the \textit{Mel Cepstral Distortion (MCD)}.
\par
Mel Cepstral Distortion \cite{kubichek1993mel} is a well described measure, that should mirror differences between speech samples.
Moreover since it considers the $mel cepstrum$ of the signal, differences in pronunciation can be captured by this method.
This means we can use it if we want to evaluate difference of two recordings objectively.
There are some caveats worth stating at the outset.
First, there are many other factors that contribute to the perception of voice quality.
For example it takes no account of speech dynamics, either short-range differentials or long-range prosodic effects.
Moreover, distortions in the pitch contour are ignored.
However, it should reflect the differences in speech quality and pronunciation.
\par
As we have described in the previous section, the audio sample can be described by its $cepstrum$.
This cepstrum may be then represented by Mel Frequency Cepstral Coefficients (MFCCs). Its order can be chosen arbitrarily, we used 35 coefficients length vectors. Thus we can transform each audio record into a sequence of float vectors with length 35. The MCD \cite{kubichek1993mel} of two sequences is basically normalized Mean Squared Error between those sequences after aligning. We define it as follows:
\begin{equation}
MCD(v^1, v^2) = \frac{\alpha}{T'}\sum_{ph(t) \neq SIL}\sqrt{\sum_{d=1}^{D}(v_d^1(t), v_d^2(t))^2}
\end{equation}
The $T'$ stands for the number of frames of the shorter of the recordings.
The common use of MCD involves omitting the $0$-th coefficient, since it corresponds to the intensity of signal and therefore it is not usually desired.
The scaling factor $\alpha$ is present for historical reasons.
\par
The MCD measure has one substantial disadvantage: it does not reflect that the two compared recordings may not have the same length and, what is more, they can be misaligned.
Thus, we use a modified algorithm that first aligns the sequences using Dynamic Time Warping and then computes MCD with the result.
\section{Finite state transducers}
\label{fst-desc}
Another concept we want to introduce here is that of \textit{Finite State Trasducers (FSTs)}.
Finite state transducers \cite{mohri1997finite}, are basically finite state automata that are augmented by addition of output labels to each transition.
We can further modify the FST and add a weight to each edge, creating a so called \textit{weighted FST}.
To go through the basic concepts of FSTs, we first have to introduce the idea of \textit{semirings}.
We define a semiring $(S,\oplus,\otimes,\overline{0},\overline{1})$ as a system that fullfills the following conditions:
\begin{nobreak}
\begin{itemize}
\item $(S,\oplus,\overline{0})$ is a commutative monoid with identity element $\overline{0}$
\item $(S,\otimes,\overline{1})$ is a monoid with identity element $\overline{0}$
\item $\otimes $ distributes over $\oplus$
\item $\forall a \in S: a \otimes \overline{0} = \overline{0} \otimes a = \overline{0}$
\end{itemize}
\end{nobreak}
With this definition of the semiring, we can describe a weighted FST \cite{mohri2009weighted} $T$ over a semiring $S$ formally as a 8-tuple $(\Sigma,\Delta,Q,I,F,E,\lambda,\rho)$ provided that:
\begin{nobreak}
\begin{itemize}
\item $\Sigma, \Delta$ are finite input and output alphabets
\item $Q$ is a finite set of states
\item $I \subset Q$ is a set of initial states
\item $F \subset Q$ is a set of final states
\item $E$ is a multiset of transitions
\item $\lambda: I \rightarrow S$ is an initial weight function
\item $\rho: F \rightarrow S$ is a final weight function.
\end{itemize}
\end{nobreak}
Each transition is element of $Q \times \Sigma \times \Delta \times S \times Q$.
Note, that since $E$ is a multiset, it allows two transitions between states $p$ and $q$ with the same input and output label, and even the same weight.
However, this is not used in practice.
An example of weighted FST is given in \figref{fst_example}.
\par
Finite state transducers have been used widely in many areas, especially linguistics.
They can represent local phenomena encountered in the study of language and they usage often leads to compact representations.
Moreover, it is also very good from the computational point of view, since it advantageous in terms of time and space efficiency.
Many algorithms has been described to efficiently work with FSTs.
\par
One of the most important is $determinization$.
This algorithm creates an equivalent FST that has the property that no state has two transitions with the same input label.
Thus it allows the time complexity of the input processing to be linear in the input length.
Weighted FSTs have been widely used in the area of automatic speech recogniton.
They allows to compactly represent the acoustic model's hypotheses state space.
Also, they can be use for combination of these hypotheses with the information contained in the \textit{Language Model (LM)}.
This is because the LM can be represented as FST as well and we can then use well described algorithms for combination of two FSTs.
\img{fst_example.png}{0.9}{Example of Finite State Transducers and their composition}{fst_example}
\section{Automatic speech recognition (ASR)}
\label{ASR-desc}
\subsection*{Overview}
The task in ASR is quite clear: An utterance spoken in natural language should be translated into its text representation.
Formally, we are given a sequence of acoustic observations $\textbf{X} = X_1X_2\dots X_n$ and we want to find out the corresponding word sequence $\textbf{W} = W_1W_2\dots W_m$ that has a maximum posterior probability $P(W | X)$i.e., according to the Bayes rule:
\begin{equation}
\hat{\textbf{W}} = argmax_w \frac{P(\textbf{W})P(\textbf{W}|\textbf{X})}{P(\textbf{X})}
\end{equation}
The problem's solution consists of two subtasks.
First, we have to build accurate acoustic model that describes the conditional distribution $P(W|X)$.
The second problem is to create a language model that reflects the spoken language that should be recognized.
Usually, a variation of the standard $N$-gram approach is sufficient.
\par
Individual words are composed of phonetic units, which are in turn modeled using states in probabilistic machinery such as a finite state transducers.
Because the speech signal is continuous, it is transformed to discrete sequence of samples.
MFFC's are commonly used to represent the signal.
The process of deriving this sequence is described in further detail in section \ref{ASP-desc}.
In general, it is difficult to use whole-word models for the acoustic part, because every new task may contain unseen words.
Even if we had sufficient number of examples to cover all the words, the data would be too large.
Thus, we have to select basic units to represent salient acoustic and phonetic information.
These units should be $accurate, trainable$ and $generalizable$.
It turns out that the most appropriate units are phones.
Phones are very good for training and generalization, however, they do not include contextual information.
Because of this, so called triphones are commonly used.
\par
To model the acoustics, Hidden Markov Models (HMM) are commonly used, because they can deal with unknown alignments.
In recent years, neural networks have experienced big breakthrough and they found application even in the area of speech recognition \cite{hinton2012deep}.
Namely, recurrent neural networks are able to work with an abstraction of memory and process sequences of variable length, so it overcomes the HMM's in terms of accuracy.
The language and acoustic models are traditionally trained independently and they are joined during the decoding process.
\par
The decoding process of finding the best matched word sequence $\textbf{W}$ to match the input speech signal $\textbf{X}$ in speech recognition systems is more than a simple pattern recognition problem, since there is an infinite number of word patterns to search in continuous speech recognition.
A \textit{lattice} is built in order to make the decoding.
A decoding lattice is a directed acyclic graph with a single start point, its nodes represent acoustic units and edges are assigned costs.
It can be represented as FST.
The assignment of the costs corresponds to the likelihood determined by the acoustic model as well as the language model.
Decoding the output is realized as searching the best path through this lattice.
Acousting units (phones) on the path compose to words and words form sentences.
\path
The decoding graph is constructed in such a way that the path can contain only words present in the vocabulary the language model is built on.
Thus, each hypothesis consists of valid words.
Because of that, we can see the word as a basic unit that can't be further split although it is actually composed from phones, or triphones respectively.
However, we can build an FST that does not restrict the paths in such way and thus allows us to construct the path from phones and recognize the input on the phonetic level.
Also it is important to note, that the search algorithm preserves alternative paths so in the end it gives us not only the best path but also a list of alternative hypotheses.
We call it the $n$-best list.
Each hypothesis in the $n$-best list is associated with its likelihood that expresses information from both the language and acoustic model.
In section \ref{nbest-detail} we explore the $n$-best list and propose method that exploits it.
\label{ASR-phn}
\section{Text-to-speech (TTS)}
\subsection*{Overview}
\cite{taylor2009text}
The text-to-speech problem can be looked at as a task of transforming an arbitrary utterance in natural language from its written form to the spoken one.
In general, these two forms have commonalities in the sense that if we can decode the form from the written signal, then we have virtually all the information we require to generate a spoken signal.
Importantly, it is not necessary to (fully) uncover the meaning of the written signal, i.e. employ Spoken Language Understanding (SLU).
It is quite clear, that we do not have to know particular words \textit{meaning} to be able to pronounce it.
However, it is good to note that.
\par
On the other hand, we may need to generate prosody which adds some information about emotional state of the speaker or emphasizes certain parts of the sentence and thus changing the meaning slightly.
To obtain prosodic information, sophisticated techniques need to be involved, since its not a part of common written text, except some punctuation.
Another difficulties stem from the fact, that we often need to read numbers, or characters with special meanings such as dates or mathematical equations.
The problem of converting text into speech has been heavily explored in the past and the TTS systems (engines) are very sophisticated nowadays.
The subsequent section, describes briefly the typical architecture of TTS systems.
\subsection{TTS system architecture}
Let us now describe a common design of TTS system.
The architecture usually consists of several modules, although some end-to-end systems base on neural networks have appeared recently (\cite{van2016wavenet}, \cite{wang2017tacotron}).
We give an overview of the typical TTS architecture in \figref{ttsarch}.
\par
First, the input text is divided into sentences and each sentence is further tokenized, based on whitespace characters, punctuation etc.
Then, the non-natural language tokens are decoded and transformed into text form.
We then try to get rid of ambiguity and do prosodic analysis.
Although much of the information is missing from the text, we use algorithms to determine the phrasing, prominence patterns and tuning the intonation of the utterance.
\par
Next is the synthesis phase.
The first stage in the synthesis phase is to take the words we have just found and encode them as phonemes.
This is an essential step and it is also of big importance for our work.
Typically, the TTS system contains a pronunciation vocabulary that is utilized in this step.
However, a situation can occur, when the desired word's pronunciation is missing in the vocabulary.
This situation is commonly refered to as the \textit{Out-of-Vocabulary (OOV) problem}.
Some technique of \textit{grapheme-to-phoneme (g2p) conversion} is usually employed at this point.
The drawback is, that transcriptions derived this way are potentionally incorrect.
This is also the point, where our method comes into play.
The derived pronunciation can be added to the pronunciation vocabulary, thus avoiding the need to use the $g2p$ and allowing to use the pronunciation directly.
\par
Once we have the phonetic transcription, the synthesis stage takes place.
The task is to create the actual acoustic signal that would correspond to the phonemes obtained previously.
There are two main approaches how to deal with this phase.
Traditional approach is so called \textit{unit concatenation}.
This approach uses a database of short prerecorded segments of speech (roughly 3 segments per phoneme).
These segments are concatenated together using some signal processing refinements so they fit together well.
Usually, each segment is recorded in several contexts so it takes some preprocessing to pick the correct variants.
\par
An alternative is to use machine learning techniques to infer the speech trajectories, represented for example as MFCCs.
Typically, the speech is modelled with Hidden Markov Models or Neural Networks that are trained on the large Corpuses of acoustic data.
Vaguely speaking, we can think of this task as the inverse version of the Speech Recognition.
However, the approach to modelling is very similair.
Trained model transforms phonetic transcription to signal representation.
So called \textit{vocoder} is then used to synthesize the actual audio signal.
\par
While both the statistical and the concatenative approach can be described as data-driven, in the concatenative approach we are effectively memorizing the data, whereas in the statistical approach we are attempting to learn general properties of the data.
Although unit concatenation approaches generally achieve better results, mainly in terms of naturalness, the statistical ones are more flexible and have good possibilities of fine tuning or post processing.
\img{TTSarch.png}{0.9}{Overview of a typical statistical Text-to-speech system architecture. The text is first analyzed and translated to phonetic transcription. Then, the acoustic parameters are estimated. Finally, the speech is synthesized.}{ttsarch}
\subsection{Grapheme to Phoneme \textit{(g2p)} conversion and its drawbacks}
\label{g2p-desc}
We now look more closely at the task of converting sequences of graphemes to corresponding sequences of phonemes.
In other words, we want to derive pronunciations from ortographic transcriptions.
Traditionally, the $g2p$ task was solved using decision trees models.
It can also be formulated as a problem of translating one sequence to another, so neural networks can be used to solve this task (\cite{yao2015sequence}).
However, in our work we use joint-sequence model (\cite{bisani2008joint}), which estimates joint distribution of phonemes and graphemes.
\par
The $g2p$ module is a crucial component of the system and it has great impact on the final pronunciation.
Since we use machine learning techniques and train on the dictionary data, the model inherently adopts pronunciation rules from the respective language.
Although it is in principle possible to  train multilingual $g2p$ (\cite{schlippe2012grapheme}), it is usually not the case in common TTS systems.
The problem arises when words that originate in some foreign language should be pronounced.
We may not be able to correctly determine the language.
Potentionally, this stage can cause errors, because the model is inherently imperfect.
\subsection*{Speech Synthesis Markup Language (SSML)}
\label{SSML}
\cite{taylor1997ssml}
SSML is an XML standard that allows specification of some properties of the synthesized speech.
It extends the text-to-speech input and many TTS engines support its use, i.e. they can process it and modify the output accordingly.
SSML allows us to specify emotions, breaks etc.
More importantly, it can be used to precisely determine the phonemes to be used and thus circumvent the phase of phonetic transcription.
In our work we use this approach to feed our phonetic transcriptions into the engine.
It should be noted, that some TTS systems does not support SSML usage.
However, in principle it is possible to add the transcriptions directly into the system's vocabulary.
\begin{center}
\begin{figure}
\textbf{\texttt{
<?xml version="1.0"?>
<speak version="1.0" xmlns="..."
         xmlns:xsi="..."
         xsi:schemaLocation="..."
         xml:lang="en-US">\linebreak
  \tab <voice gender="female">Mary had a little lamb,</voice>\linebreak
  \tab <voice gender="female" variant="2">\linebreak
  \tab\tab Its fleece was white as snow.\linebreak
  \tab</voice>\linebreak
  \tab<voice name="Mike">I want to be like Mike.</voice>\linebreak
</speak>
}}
\label{ssml_example}
\caption{Example of SSML code}
\end{figure}
\end{center}

\subsection*{Overview of the used TTS systems}
\begin{enumerate}
\item Cereproc\footnote{https://www.cereproc.com/}
This engine is a commercial software that is free for educational purposes.
Although it is not open-sourced, we use it for its good quality and reliable outputs.
\item MaryTTS\footnote{http://mary.dfki.de/} MaryTTS is a German open-source project written in Java.
It is highly customizable and modular.
Thus we can explore output of an arbitrary module or replace it in the processing pipeline.
MaryTTS is based on client-server architecture.
\item gTTS\footnote{https://pypi.python.org/pypi/gTTS} is a TTS service provided online by Google.
It achieves very good quality, however it allows ony use of one voice in the free version.
\item Pico \footnote{https://github.com/stevenmirabito/asterisk-picotts} SVOX Pico TTS is a lightweight engine that lacks a good quality of the gTTS, however it offers a large selection of voices and it is commonly used on the phones with Google's Android operating system.

\end{enumerate}
\section{Algorithms description}
\subsection{Dynamic Time Warping (DTW)}
In our work, we frequently run into the subsequent problem:
We want to somehow process two sequences - find similarities (or differences), explore characters on corresponding positions, etc.
The problem is, that the sequences are typically of different lengths.
Luckily, this issue has to be solved in many applications, so it has been explored already.
One option is to use the \textit{Dynamic Time Warping (DTW)}~\cite{ratanamahatana2004everything} algorithm.
\par
Suppose we have two time series, a sequence $Q$ of length $n$,
and a sequence $C$ of length $m$, where
\begin{center}
$Q = q_1 ,q_2 ,\dots, q_i ,\dots ,q_n$
\linebreak
$C = c_1 ,c_2 ,\dots, c_j \dots, c_m$
\end{center}
To align these two sequences using DTW, we first construct an \textit{n-by-m} matrix where the $(i^{th} , j^{th} )$ element of the matrix correspond to the squared distance $d(q_i, c_j) =
(q_i - c_j)^2$ .
To find the best match between these two sequences, we retrieve a path through the matrix that minimizes the total cumulative distance between them.
Methods of dynamic programming are used to fill in the values and find alignment of sequences.
Thus a result of this algorithm is a mapping $\sigma$ that can be used to construct sequence of pairs $A$ containing members of both sequences $C,Q$ in each time step.
$\sigma$ is constructed in such a way, that it holds:
\begin{equation}
A_i = (C_{\sigma (C,i)}, Q_{\sigma (Q, i)}); i = 1 \dots K
\end{equation} 
\begin{equation}
max(|C|,|Q|) \le K
\end{equation}
\begin{equation}
\sum_{i=0}^K d(\sigma (C,i), \sigma(Q,i) is minimal
\end{equation}
Where $d(x,y)$ is an arbitrary distance metric.
In our application, we want to align two sequences of phonemes, hence we can treat it as strings and work with respective distance.
We choose Levenshtein distance to be our metric.
\subsection{Clustering}
Cluster analysis or clustering \footnote{https://en.wikipedia.org/wiki/Cluster\_analysis} is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other clusters.
The measure of similarity can be arbitrary distance measure.
Usually, euclidean distance is used when considering points in an euclidean space, but it is not a requirement.
Clustering is inherently an unsupervised method, we often want to describe unknown data, that is find some structure or local similarities.
\par
We mention the clustering algorithms here, since we use them to find groups (clusters) of mutually similair hypotheses in the automatic speech decoder's $n$-best list.
In section \ref{nbest-detail} we argue, that a lot of hypothesis differ from each other only slightly.
We aim to extract only important hypotheses that are not so similair by clustering them.
There exist different methods of clustering.
We now describe some of them shortly.
\par
\textit{Hierarchical clustering} is based on the core idea of objects being more related to nearby objects than to objects farther away.
These algorithms connect objects to form clusters based on their distance.
Hierarchical clustering is a bottom-up method, meaning that it forms clusters iteratively.
The hierarchical clustering algorithms' result can be represented as a $dendrogram$, which is a tree diagram representing relations among the explored data.
An example of dendrogram is given in \figref{dendro}.
We can make a horizontal cut to derive respective clusters.
\img{dendrogram.png}{0.9}{An example of dendrogram outputted by hierarchical clustering methods.}{dendro}
\par
On the other hand in \textit{Centroid-based clustering}, clusters are represented by a central vector, which may not necessarily be a member of the data set.
If we fix the number of clusters to $k$, we can see the task as an optimization problem: find the $k$ cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
Typical algorithm from this group is $k$-means clustering\footnote{https://en.wikipedia.org/wiki/K-means\_clustering}. which is an Expectation-Maximization (EM) algorithm.
\par
\textit{Spectral clustering} \cite{ng2002spectral}
The task of finding good clusters has been the focus of considerable research in machine learning and pattern recognition.
Often, \textit{Expectation-Maximization (EM)} algorithms are used to learn a mixture density.
The EM algorithms estimate parameters of the model describing some data.
The model uses latent (unobserved) variables that has to be estimated.
The algorithms work by iterating two steps - the \textit{expectation} step and the \textit{maximization} step.
In the expectation (E) step, a function for computation of expectation of the log-likelihood of the data is created using the current estimate for the parameters and in the maximization (M) step the parameters are updated to maximize the function.
\par
Unfortunately, these approaches have several drawbacks.
First, to use parametric density estimators, simplifying assumptions usually need to be made (e.g., that the density of each cluster is Gaussian).
Second, the log likelihood can have many local minima and therefore multiple restarts are required to find a good solution using iterative algorithms. 
An alternative is to use spectral methods for clustering.
Here, the top eigenvectors of a matrix derived from the distance between points are used.
Therefore, the mentioned problems are solved.
\pagebreak
\subsection{Notions in speech communication}
In this work, we consider two forms of utterances, spoken and written and their relationship.
However, the concept of \textit{form} needs some elaboration and formal intorduction.
Imagine, someone wants to deliver an information by showing some color.
If a red card is used, it isn't the card itself that is the important point, rather than the red color was used instead of different color.
So for example, a different red card could be used successfully  as it
is not the particular red card itself which is the form, but rather that the card has the property "red" and it is this property and its contrasting values that is the basis of the communication system.
\par
We use the term signal to refer to the physical manifestation of the form.
Here the term signal is again used in a specific technical sense: it can be thought of as the thing which is stored or the
thing which is transmitted during communication.
The idea of "red" and the fact that it contrasts with other colors is the form; the physical light waves that travel are the signal.
\par
It is important to see that the relationship between meaning and form, and form and signal
are quite separate.
If we assigned meaning only to colors red, blue and green and an orange card would have been showed to us, we would not decode the meaning, although the signal-form translation worked well.
This situation is similar to that when we read a new word - while are quite capable of using our eyes to read the physical pattern from the page and create an idea of the letters in our head, if we don't know the form-meaning mapping of the word, we can not understand it.
Finally, we use the term channel or medium to refer to the means by which the message
is converted and transmitted as a signal.
\par
In this work, we deal mainly with the spoken channel and the written channel.
Encoding is the process of creating a signal from a message. When dealing with speech, we talk
of speech encoding and when dealing with writing we talk of writing encoding. Speech encoding
by computer is more commonly known as speech synthesis, which is of course the topic of this
book.
The most significant aspect of speech encoding is that the nature of the two representations,
the message and the speech signal, are dramatically different.
A word (e.g. $HELLO$) may be composed of four phonemes ($/H\:EH\:L\:OW/$) but the speech signal is a continuously varying acoustic waveform, with no discrete components.
In this work, we focus on derivation of the first representation and we let the used synthesis frameworks do the speech signal generation.
So we simplify the speech representation to its phonetic transcription.
\par
However, it is important to understand, that we do not change the meaning or even form of the communication, we just want to use different channel for the transmition.
When we consider the verbal language, we may talk about the duality of form such that we have phonemes which combine to words which combine to form sentences.
We define three terms that help us to describe the problematics:
\begin{enumerate}
\item \textbf{phonemes} are members of the relatively small set of units which can be combined to produce distinct word forms.
\item Term \textbf{phone} is used to describe a single speech sound, spoken with a single articulatory configuration.
\item finally, different ways of realizing a single phoneme are called \textbf{allophones}.
\end{enumerate}
The duality principle states that verbal language is not one system but two; the first system uses a
small inventory of forms, phonemes, which sequentially combine to form a much larger inventory
of words.
\par
We have a similar dual structure in written communication, where words are made of \textbf{graphemes}.
Graphemes are in many ways analogous to phonemes, but differ in that they vary much more from language to language.
We consider alphabetic languages in this work, encoded with the Latin alphabet\footnote{https://en.wikipedia.org/wiki/Latin\_alphabet}.
However, it is possible to extend the used grapheme-to-phoneme conversion technique \cite{bisani2008joint} to work with other types of languages.
\par
To encode the phonemes, phonetic alphabets are used.
The standard is an International Phonetic Alphabet (IPA)\footnote{https://en.wikipedia.org/wiki/International\_Phonetic\_Alphabet}.
IPA symbols are composed of one or more elements of two basic types, letters and diacritics.
The general principle of the IPA is to provide one letter for each distinctive sound (speech segment), although this practice is not followed if the sound itself is complex.
An example of IPA characters is given in \figref{ipa_overview}.
\img{ipa_overview.png}{0.9}{Some letter of the IPA alphabet together with their occurrences in English words.}{ipa_overview}
\pagebreak
\section{Related Work}
\label{relatedwork}
\subsection{Grapheme-to-phoneme conversion}
An automatic grapheme-to-phoneme conversion was first considered in the context of TTS applications. The input text needs to be converted to a sequence of phonemes which is then fed into a speech synthesizer.
It is common in TTS systems that they first try to find the desired word in the dictionary and if it doesn't find it, it employs the grapheme-to-phoneme ($g2p$) module.
A trivial approach is to employ a \textit{dictionary look-up}.
However, it cannot handle context and inherently covers only finite set of combinations.
To overcome this limitations, the rule-based conversion was developed.
Kaplan and Kay \cite{kaplan1994regular} formulate these rules in terms of finite-state automata.
This system allows to greatly improve coverage.
However the process of designing sufficient set of rules is difficult, mainly since it must capture irregularities.
Because of this, a \textit{data-driven} approach based on machine learning has to be employed.
Many such techniques were explored, starting with Sejnowski and Rosenberg \cite{sejnowski1988nettalk}.
The approaches can be divided into three groups.
\subsubsection{Techniques based on local similarities}
The techniques presuppose an alignment in the training data between letters and phonemes or create such an alignment in a separate preprocessing step.
The alignment is typically construed so that each alignment item comprises exactly one letter.
Each slot is then classified (using its context) and a correct phoneme is predicted.
Neural networks and decision tree classifiers are commonly used for this task.
\subsubsection{Pronunciation by analogy}
This term is typically used for methods that could be described as nearest-neighbor-like.
They search for local similarities in the training lexicon and the output pronunciation is chosen to be analogous to retrieved examples. 
In the work of Dedina and Nusbaum \cite{dedina1991pronounce} the authors first identify words from the database that match the input string and then build a pronunciation lattice from them.
Paths through the lattice then represent the derived pronunciations.
\subsubsection{Probabilistic approaches}
The problem can also be viewed from a probabilistic perspective.
Pioneers in this area were Lucassen and Mercer \cite{lucassen1984information}.
They create $1-to-n$ alignments of the training data using a context independent channel model.
The prediction of the next phoneme is based on a symmetric window of letters and left-sided window of phonemes.
Authors then construct regression tree, the leafs of which carry probability distribution over phonemes.
\linebreak\linebreak
\label{g2p-jseq}
A popular approach based on probability modeling is to employ so called \textit{joint sequence models} \cite{bisani2008joint}.
We use an open-source implementation of such a model in our work.
This approach formalizes the task as follows:
\begin{equation}
\boldsymbol{\varphi(g)} = \argmax_{\varphi'\in\Phi^*} p(\boldsymbol{g,\varphi'})
\end{equation}
where * denotes a Kleene star.
In other words, for a given ortographic form $\boldsymbol{g} \in G^*$ we want to find the most likely pronunciation $\boldsymbol{\varphi} \in \Phi^*$, where $G, \Phi$ are ortographic and phonetic alphabets.
The fundamental idea of joint-sequence models is that the relation of input and output sequences can be generated from a common sequence of joint units.
These units carry both input and output symbols.
Formally, the unit called \textit{grapheme} is a pair $q = (\mathbf{g}, \mathbf{\varphi}) \in Q \subset G* \times \Phi$ where $\mathbf{g}$ and $\mathbf{\varphi}$ are letter and phoneme sequences which can be of different lengths.
In the simplest case, each unit carries zero or one input and zero or one output symbol.
This corresponds to the conventional definition of finite state transducers (FST) that are described in \ref{fst-desc}.
The letter and the phoneme sequences are grouped into an equal number of segments.
Such a grouping is called a joint segmentation.
The joint probability is defined as:
\begin{equation}
p(\mathbf{g}, \mathbf{\varphi}) = \sum_{\mathbf{q}\in S(\mathbf{g},\mathbf{\varphi})}p(\mathbf{q})
\end{equation}
since there are many possible groupings of input sequences in general.
The joint probability distribution $p(\mathbf{g}, \mathbf{\varphi})$ has thus been reduced to a probability distribution $p(\mathbf{q})$ over graphone sequences $\mathbf{q} = q_1, \dots, q_K,$ which can be modeled using  $N$-gram approximation:
\begin{equation}
p(q_1^K) \cong \prod_{j=1}^{K+1} p(q_j\vert q_{j-1},\dots,q_{j-M+1})
\end{equation}
The probability distribution is then typically estimated by an Expectation-Maximization algorithm.
\linebreak \linebreak
Generally, the problem of the wrong pronunciation in TTS  is caused by a bad phonetic transcription.
Traditional TTS systems are modular, one module's output is inputted into the next one.
Because of this fact, the errors cumulate and thus the mistakes made by $g2p$ cannot be repaired.
So if we want to improve the pronunciation, we can try to improve the $g2p$ as it is done in \cite{deri2016grapheme}.
Authors in this work propose a method of exploiting a $g2p$ trained on a language with a high number of available resources to create a $g2p$ for language for which we do not have sufficient number of examples.
This method relies on the existence of a conversion mapping between these two languages.
Also it requires to do the conversion for every new language.
In theory, a model can be created, that is able to transcribe grapheme sequences into an appropriate phonetic representation and can handle multiple languages.
However, it needs to somehow obtain information, which language the input sequence comes from, which is not straightforwardly doable.
This method has several drawbacks, because the language is not always known and the set of known languages is limited, so it does not really solve the OOV problem.
Also, it potentially requires a lot of training data.
Moreover, if we want to learn a new pronunciation of just one word, it's more convenient to do it in a different way.
\subsection{Learning pronunciation from spoken examples}.
\label{pronunc-spoken}
This group of methodologies aim to derive phonetic transcriptions directly from audio input.
They are built on the theory of Automatic Speech Recognition which we discuss in \ref{ASR-desc}.
Authors of \cite{slobada1996dictionary} introduce method of deriving correct pronunciation for a word in order to enlarge the recognizer's dictionary.
They propose a data-driven approach to automatically add new words and their variants, respectively.
The authors argue that in the spontaneous speech, the most frequent pronunciation does not need to be the one that is marked as correct and is used in the training phase.
Thus the overall performance of the recognizer may be degraded since the phonetic units are bound with inadequate acoustics.
The method is proposed, that relies on the use of both phoneme and word-level speech recognizer.
The phoneme recognizer is constructed using smoothed bigram Language model.
We discuss the phoneme recognizers in more detail in \ref{ASR-phn}.
The algorithm collects all occurrences of words in the database and creates phonetic transcriptions using the recognizer.
It then sorts the variants, rejects some of them an creates an $n-best$ list which is added to the dictionary.
The recognizer can then be retrained, allowing multiple pronunciations for each word.
Thus the recognition performance can be improved by an automatic procedure without the need for using the phonological rules.
\linebreak\linebreak
In the work of McGraw et al. \cite{mcgraw2013learning}, the concept of Pronunciation Mixture Models is introduced.
The authors use a special kind of speech recognizer, the search space of which has four primary hierarchical components: a language model $G$, a phoneme lexicon $L$, phonological rules $P$ that expand the phoneme pronunciations to their phone variations, and a mapping from phone sequences to context-dependent model labels
$C$.
All of the components mentioned can be with advantage represented as FSTs and thus the full decoder network can be represented as a composition of these components: $R = C\circ P \circ L \circ G$.
A probabilistic lexicon is considered, meaning, that several pronunciations are allowed for each word and there is no hard limitation that would force the recognizer to choose one.
Instead, a kind of soft voting is considered, meaning, that each transcription is used with certain probability.
Also, joint-sequence modeling is considered, as we introduced in \ref{g2p-jseq}, so each transcription is considered together with its orotgraphic form.
That means, that we can describe the log-likelihood of $M$ utterances $ D = \{\mathbf{u}_i,\mathbf{W}_i\}$ where $\mathbf{u}_i$ are speech data and $\mathbf{W}_i$ their transcriptions as follows:
\begin{equation}
\mathcal{L}(\Theta\vert D) = \sum_{i=1}^M log \sum_{B \in \mathcal{B}} P(\mathbf{u}_i,\mathbf{B},\mathbf{W}_i;\Theta)
\end{equation}
where $\mathbf{B}$ are respective phone sequences (i.e. pronunciations) and $\Theta$ represents the model parameters.
Then, we derive using a chain rule:
\begin{equation}
P(\mathbf{u}_i,\mathbf{B},\mathbf{W}_i;\Theta) =  P(\mathbf{u}_i\vert\mathbf{B})P(\mathbf{B}\vert\mathbf{W}_i;\Theta)P(\mathbf{W}_i)
\end{equation}
If we further assume, that pronunciation sub-units $\mathbf{b}_i \in \mathbf{B}$ are context independent, we can transcribe the above expression:
\begin{equation}
P(\mathbf{u}_i\vert\mathbf{B})P(\mathbf{B}\vert\mathbf{W}_i;\Theta)P(\mathbf{W}_i) = P(\mathbf{u}_i\vert\mathbf{B})(\prod_{j=1}^{k_i}P(\mathbf{b}_j\vert \mathbf{w}^i_j;\Theta))P(\mathbf{W}_i) 
\end{equation}
The model parameters are then estimated using the EM-algorithm.
Parameters related to the language model, can be initialized with use of graphone language model.
Several technical issues has to be dealt with, however the Pronunciation Mixture Models can be trained on the same data as traditional ASR engines and can be used to obtain phonetic transcriptions from audio data.
\linebreak\linebreak

Similar approach is considered in the work of Reddy and Gouvea \cite{reddy2011learning}, except they do not have access to acoustic models or phone lattices.
They use only the mistakes done by the recognizer as their source of information.
An OOV word is passed through an ASR decoder giving an $n$-best word recognition output.
Since the words are OOVs, every hypothesis will be a recognition mistake.
These mistakes are then exploited, assuming that the following generative story of the recognition output
for a word $\mathbf{w}$ holds:
\begin{enumerate}
\item A pronunciation baseform $\mathbf{b}$ is drawn from the distribution $\Theta$.
\item A phonetic confusion function from the word $\mathbf{w}$ and the selected baseform $\mathbf{b}$ is applied in order to generate a phoneme sequence $\mathbf{p}$ with probability $P(\mathbf{p}|\mathbf{b},\mathbf{w})$
\item A word sequence $\mathbf{e}$ with probability $P (\mathbf{e} \vert \mathbf{p},\mathbf{b},\mathbf{w}) = 1$ 
is generated using the pronunciation lexicon.
\end{enumerate}
The authors model the joint probability of hypothesis and reference word $P(\mathbf{e}, \mathbf{w}) = P(\mathbf{w}\sum_b f_{e,b,w})$, where $f_{e,b,w} = P(\mathbf{e}\vert \mathbf{b}, \mathbf{w})$ is the phonetic confusion function and it is used to estimate the distribution $P(\mathbf{b}\vert\mathbf{w},\mathbf{e})$.
Thus they are able to derive pronunciations without access to ASR lattices, i.e. it only consideres the recognizer as a black-box. 
\subsection{Conclusion}
Many approaches were introduced that are able to convert an utterance in ortographic or audio form to its phonetic representation.
$G2P$ converts the grapheme transcriptions, using only the text input.
It is a well explored field of study with many different variants of realization.
Although it achieves very good results nowadays, it suffers from the fact that same groups of letters may have different pronunciations in different languages.
We typically don't have access to the information which language is considered and it may be difficult to get access to sufficient number of datasets.
The latter problem can be partially solved by transfer learning as proposed in \cite{deri2016grapheme}.
\linebreak\linebreak
Alternatively, one can derive pronunciations directly from an audio signal.
This approach has been also explored by some authors, however it usually requires quite low-level modifications of the speech recognizer.
Also, the authors used the derived pronunciations to enlarge the phonetic dictionary of the recognizer, not to improve the Text-To-Speech systems.
We explore methods of merging the mentioned approaches to combine both textual and acoustic information and its usability when confronted with human judgments.