\chapter{Proposed approaches}
\section{Basic overview}
\label{basover}
We want to deal with the problem of Out-of-Vocabulary (OOV) words in Text-to-speech and potentionally the ASR systems.
Our goal is to improve the phase, where the pronunciation in form of phonetic transcription is derived.
That means, we need to derive phonetic transcriptions of arbitrary words, correct pronunciation of which is unknown to us.
The straightforward way of achieving this is to use a \textit{grapheme-to-phoneme} ($g2p$) converter.
As we discuss in sections \ref{g2p-desc} and \ref{relatedwork}, this approach has some limitations, mainly because the pronunciation of some tokens in different languages may vary.
This means, that $g2p$ may provide pronunciation which would be correct if the world came from certain language $l$, but since it is not true, the pronunciation is actually wrong.
However, \textit{grapheme-to-phoneme conversion} is a well explored area and its output may serve as a sufficiently good baseline.
\par
In section \ref{relatedwork} we also discuss methods that are able to derive phonetic transcriptions directly from the speech signal, using a speech recognition framework. These methods are somewhat complex and they are typically specific for a particular recognizer. Rather than improving any of these methods, we aim to explore a possibility to combine them. That is, we want to exploit both textual and acoustic information to derive phonetic transcriptions of the desired words.
\par
\label{detection}
The question is, how to get audio samples containing the correctly pronounced spoken versions.
One option is to make the best effort when pronouncing the words.
In case that we pronounce some of the words incorrectly, we rely on the user's reaction, i.e. we expect he informs us about the mistake and subsequently provide a correct example.
Other option we have is to detect possible faults apriori and ask the user for the correct pronunciation directly.
Whole process may look like this:
\begin{enumerate}
\item Lookup the desired word in the pronunciation lexicon. In case it is not found here proceed to the classification step:
\item Decide if the word is difficult to pronounce for our Text-to-speech system.
\item If it is marked as difficult, ask the user for correct pronunciation.
\end{enumerate}
In the desired settings, the acoustic data with user's recordings are obtained online.
We mean by this a situation, when the system asks user for a correct pronunciation directly.
It can be difficult to make user say words we are interested in, so the ultimate goal is to develop an algorithm able to extract those words from a dialogue history or ask user to say them in a not irritating way.
The Crucial point of this approach is, that the user should not feel bored by the process. For the purpose of this work, we assume that we have already obtained recordings with correct pronunciations and work with it.
In the real setting we would have to employ a dialogue policy to be able to gather our own recordings or process the history and try to identify respective words.
\section{Used techniques}
\subsection{Phoneme recognition}
\label{ph-rec}
To improve the pronunciation, it is essential to process the user's recording and be able to obtain essential information from it.
We need to work on the phonetic level, so we have to get phonetic transcription of the recording.
In theory, we could use the standard output of the speech recognizer, i.e. a sequence of words and transcribe it phonetically.
This has two essential problems:
\begin{enumerate}
\item Such procedure adds new source of potential errors, because we would have to use some mechanism, $g2p$ module for instance, that would create the transcription. This mechanism is error prone and we do not have a reliable one - if we had, we would not have to exploit the recording in the first place.
\item Furthermore, since we aim to add new words to the TTS vocabulary, it is likely, that the word is missing also in the vocabulary of the speech recognition system.
This fact means that the desired word will not appear in the recognizer's output, different word will appear instead.
Thus we cannot obtain any relevant transcription.
\end{enumerate}
Instead of this, we can modify the decoder used in the speech recognizer in such way, that it outputs sequences of phonemes rather than words.
This can be done in several ways, some of which we introduce in the following text.
\subsubsection{On the Automatic Speech Recognition (ASR) decoding process}
\label{asr-decoding}
We use the Kaldi framework\footnote{http://kaldi-asr.org/} for the ASR task and follow the procedure suggested in the official documentation.
Kaldi constructs a Finite State Transducer (FST) for purposes of the decoding process\footnote{http://kaldi-asr.org/doc/graph.html}.
This transducer is created as a composition of four other FSTs, $HCLG = H \circ C \circ L \circ G$.
The meanings of the parts are as follows:
\begin{itemize}
\item $G$ is an acceptor (i.e. its input and output symbols are the same) that encodes the grammar or language model.
\item $L$ is the lexicon; its output symbols are words and its input symbols are phones.
\item $C$ represents the context-dependency: its output symbols are phones and its input symbols represent context-dependent phones, i.e. windows of $N$ phones
\item $H$ contains the acoustic model's definitions; its output symbols represent context-dependent phones and its input symbols reference the model.
\end{itemize}
As we can see, the $G$ FST brings information contained in the language model to the decoding graph.
We provide an example in \figref{Lfst} which shows, that the $L$ transducer determines the output elements.
Hence, by modification of $G$ and $L$ we can change the output set of the decoded tokens.
\img{Lfst.png}{0.9}{An example of the finite state transducer representing the lexicon\cite{mohri2002weighted}}{Lfst}
\subsubsection{Modifying the Language Model}
Language Model (LM) is an essential part of every ASR engine.
It provides probabilities that a particular word occur in the considered context. 
For example we can imagine a model that assigns probability to every possible sequence of words.
The length of the considered sequence can be restricted to $n$.
Such sequences are usually referred to as \textit{n-grams}.
The easiest approach is to use a so called 0-gram LM.
It simply distributes the probability mass uniformly among the words, i.e. each word occurs with same probability, independently of the context.
\par
Obviously, this model is not very good.
If we want to use more sophisticated approach, we can use collected textual data and estimate probabilities of word $n$-grams.
Since we consider phonemes, we can construct a Language Model that describes probabilities of phoneme occurrences instead of words.
\par
We can then modify $L$ and $G$ transducers to change the output of the recognizer to phonemes.
Ideally, we would use a set of name transcriptions to train the language model because it enrich the model with the domain information.
However, it is difficult to get corpus that consists of phonetic transcriptions and yet is large enough to robustly estimate the LM parameters.
So we create an artificial training dataset by taking an ordinary corpus of words and transcribing with a $g2p$ module.
\par
Alternatively, we could use phonetic alignments that are created during the decoding process and can be extracted from the recognizer.
The phonetic alignments are derived during the decoding, it holds information, which phonemes were used.
However, the Automatic Speech Recognition decoder in its common form restricts the output to contain only words that are present in its vocabulary.
The phoneme sequences corresponds to this words and therefore no unknown word (phoneme sequence) can appear in this alignments.
In other words, that transcriptions are influenced not only by the acoustic information but also by the Language Model.
Therefore, this approach is not applicable for us.
\par
Creation process of the decoding graph works with vocabulary and phonetic dictionary.
The vocabulary contains a list of known words and the dictionary respective phonetic transcriptions.
To change the transducers in the above (\ref{asr-decoding}) mentioned way, we reduce the vocabulary to list of phonemes and setting the dictionary mapping to be identity.
This way we force the decoder to output phonemes.
\subsubsection{Timit dataset}
The previous approach has a disadvantage that it is difficult to obtain a Language Model of good quality without a proper corpus of transcriptions.
Such corpus may be found as a part of the Timit dataset \cite{lopes2011phoneme}.
In fact, the Kaldi framework contains scripts, that train a model using this dataset.
The Timit dataset has got transcriptions labeled with granularity of individual phonemes.
This means we can train the phonetic recognizer directly.
Problem is that different set of phonemes is used but we can solve it by mapping the phonemes to IPA. 
\subsubsection{Modeling phoneme bigrams}
The decoding graph is context dependent. 
It means that the decoder takes into account both the right and the left context of the phonemes. 
In order to model the pronunciation on various positions in the word, each phoneme is represented in several variants, depending in which part of the word it is located (\textit{beginning, end, inside}). 
One option is to model each phoneme as a singleton, that is context-independent unit, as we have done in previous.
Alternatively, we can create a decoding graph, that models bigrams of phonemes.
This way the information about the context is preserved.
Moreover, we can model the input more accurately and distinguish pauses between words from spaces between phonemes.
This approach proved to yield the best results.
\subsubsection{Evaluation of phoneme recognizers}
\label{p-eval}
In ASR systems, the most common phone recognition evaluation measures are Phone Error Rate (PER), or the related performance metric, Phone Accuracy Rate.
The latter is defined by the following expression:
\begin{equation}
Accuracy = \frac{N_T - S - D - I}{N_T} \times 100\%
\end{equation}
where $N_T$ is the total number of phonemes in the gold utterance and $S, D$ and $I$ are the substitution, deletion and insertion errors, respectively.
From this perspective, we can see it as related to Levenshtein distance of the refrence and test sequences.
Then, we can define $PER \: = \: 100\% - Accuracy$.
\par
Another measure is correctness, which is similar to accuracy, but does not consider insertion errors.
\par
The number of insertion, deletion and substitution errors is computed using the best alignment between two token sequences: the manually aligned (reference) and the recognized (test).
An alignment resulting from search strategies based on dynamic programming is normally used successfully for a large number of speech recognition tasks \cite{ney2000progress}.
We use a $sclite$ utility\footnote{http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm} to measure PER.
\subsubsection{Exploring the speech recognizer}
\label{nbest-detail}
The decoding process in Automatic Speech Recognition yields a list of hypotheses, ordered by their confidence scores, so called $n$-best list.
Usually, the best hypothesis is chosen and used since it is the most relevant result.
However, interesting information may be found deeper in the list.
We explore this assumption in the following paragraphs.
\par
In section \ref{p-eval} we have introduced Phone Error Rate (PER).
We extend the term here and define \textbf{Oracle Phone Error Rate}:
\begin{equation}
OPER = \minop_{h \in hypotheses} PER(h, gold)
\end{equation}
We can see that Oracle Phone Error Rate is PER of the best hypothesis (if we consider the hypothesis with the lowest PER as the best one.)
In real setup we obviously does not have such oracle that would tell us which hypothesis is actually the best
So if we want to evaluate it, we have no other option but look at the top of the $n$-best list, compute PER for each hypothesis and choose the closest one.
Hence we approximate the true Oracle PER, since if we explore "sufficiently many" best alternatives, the probability that we did not see the best one is very low.
We give an overview of our results in \figref{OPER}.
\img{OPER.png}{0.9}{A plot of Oracle Phone Error Rate dependency on the depth of the $n$-best list we explore. We can see, that the Oracle PER decreases with the depth, so it makes sense to try to exploit the hypotheses located deeper.}{OPER}
\par
Using Oracle Phone Error Rate to evaluate the recognizer is basically cheating, since in real decoding process, we are not given the gold transcription which we could use to pick the best hypothesis.
Nevertheless, it indicates some properties of the decoder, namely that the hypothesis located at the top is not always the best one.
We explore this phenomenon further by looking at the particular position of the closest (best) hypothesis.
In \figref{besthyp} we plot the histogram showing, number of times the closest hypothesis appeared in particular depth.
We can see, that relevant hypotheses can be found in bigger depth.
\img{besthypdistr.png}{0.9}{Histogram that visualizes the distribution of the hypothesis, that is the closest to the gold reference utterance in terms of Phone Error Rate. The bars show the number of times the best hypothesis was found on the particular position. Two settings are included in this figure: the green bars describe situation, when first five positions were used, the blue ones were measured using nine positions}{besthyp}
\par
The observations we have made in the previous text indicates that there is a substantial piece of information contained in the phoneme recognizer's $n$-best list.
The problem of the $n$-best list is, that many pairs of hypothesis differs only in a few positions.
That is because of the nature of the procedures used for the decoding.
It is quite intuitive, that hypothesis that differ only in one position have similar probabilities.
We would like to use the information but we are not interested in many hypotheses that are nearly the same.
So we propose to clusterize the hypotheses contained in the $n$-best list.
The goal is to obtain a more compact version of the list with high variability among items.
We address this issue again in section \ref{nbest-clust}, giving results and examples.  
\section{Text-to-speech systems' evaluation}
\label{tts-eval}
The TTS evaluation\cite{huang2001spoken} is a desired task since we often want to choose between two or more engines.
However the task introduces several challenges some of which we mention in this section.
We need to define a metric in order to evaluate the TTS.
Such a metric will be dependent on particular task's setting and may consist of several variables.
We can describe a taxonomy of different evaluation methods, depending from which point of view we look at it we distinguish between:
\begin{itemize}
\item \textit{Black box vs. Glass box evaluation} - whether we evaluate the TTS system as a whole or look at particular modules of it.
\item \textit{Human vs. automated} There are two fundamentally distinct ways of evaluating
speech synthesizers: one is to use human subjects; the other to automate the evaluation process.
Nowadays, it is necessary to employ humans in the evaluation process, mainly to evaluate the quality of integration and functionality of the system.
\item \textit{Global vs. analytic} approach tell us whether we rate global aspects such as naturalness and overall quality or more analytic aspects like vowel and consonant clarity, tempo or appropriateness of stresses and accents.
\end{itemize}
\par
A critical measurement of a TTS system is whether or not human listeners can understand
the text read by the system. So called \textit{intelligibility} tests were developed to measure this aspect.
For example Diagnostic Rhyme Test is used that uses set composed of pairs of similar words.
Those are synthesized with a TTS engine and the listeners try to correctly distinguish between them.
The intelligibility is the evaluated and compared to listening human speakers.
An example set of words that may be used for such test is given in \figref{rhymetest}.
\img{rhymetest.png}{0.9}{An example set of words used to perform the Diagnostic Rhyme Test}{rhymetest}
\par
While a TTS system has to be intelligible, this does not guarantee user acceptance, because
its quality may be far from that of a human speaker.
That is the reason, why methods like \textit{Mean Opinion Score (MOS)} are frequently used.
MOS is human-subject judgment testing that asks the subjects to rate several samples on the scale of 1 to 5 (1 = Bad, 2 = Poor, 3 = Fair, 4 = Good, 5 = Excellent).
The scores are averaged, resulting in an overall MOS rating.
We have used this method of labeling to annotate our sample dataset.
\par
Normalized MOS scores for different TTS systems can be obtained without any preference judgments.
Nevertheless, sometimes comparisons are desired, especially for systems that are informally judged to be fairly close in quality.
So called Category Rating (CCR) method, may be used.
In this method, listeners are presented with a pair of speech samples on each trial. The order of the system A system B samples is chosen at random for each trial, providing, that half of the trials, the system A sample is followed by the system B and the order is reversed in the remaining trials.
Listeners rate the pair with labels meaning, that the second utterance is:
\begin{itemize}
\item 3 - much better
\item 2 - better
\item 1 slightly better
\item 0 about the same
\item -1 slightly worse
\item -2 worse
\item -3 much worse
\end{itemize}
Sometimes the granularity can be reduced as much as simply "prefer A/prefer B".
We use this method to tell which transcription is better.
\section{Evaluating classifiers}
Basic measure that is used to evaluate classifiers' performance is \textit{accuracy}.
It is a fraction of correctly classified examples over the total number of examples.
However, more descriptive measures are need sometimes.
In this work, we use the \textit{F-score}, which is computed using terms \textit{precision} and \textit{recall}~\cite{sokolova2006beyond}.
To explain this terms, we first need to introduce some others.
Suppose, we have binary classification task, that is we want to identify positive and negative examples.
In this settings, four types of situations may occur:
\begin{itemize}
\item \textit{True Positive(TP)} - classifier identified positive example as positive
\item \textit{False Positive(FP)} - classifier identified negative example as positive
\item \textit{True Negative(TN)} - classifier identified negative example as negative
\item \textit{False Negative(FN)} - classifier identified positive examples as negative
\end{itemize}
\par
With this terminology, we can define:
\begin{equation}
precision = \frac{TP}{TP + FP}
\end{equation}
\begin{equation}
recall = \frac{TP}{TP + FN}
\end{equation}
In other words, \textit{precision} describes how many selected examples are relevant, while \textit{recall} tells us how many relevant examples were actually selected.
Finally, we can define:
\begin{equation}
F-score = 2 \times \frac{precision \times recall}{precision + recall}
\end{equation}
That is, the \textit{F-score}, sometimes called \textit{$F_1$ score} is a harmonic mean of \textit{precision} and \textit{recall.}
\section{Evaluating human ratings}
When considering human ratings, we should evaluate how individual ratings agree.
One option is to compute so called \textit{Cohen's kappa coefficient}\cite{cohen1968weighted}.
It takes into account the possibility of the agreement occuring by chance.
Cohen's kappa measures the agreement between two raters who classify items into mutually exclusive categories.
The definition of $\kappa$ is:
\begin{equation}
\kappa = \frac {p_{o}-p_{e}}{1-p_{e}}=1-\frac {1-p_{o}}{1-p_{e}}
\end{equation}
where $p_o$ is the relative observed agreement among raters (identical to accuracy), and $p_e$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly seeing each category.
Interpretation is, that $\kappa = 1$ means complete agreement and $\kappa \leq 0$ if there is no agreement among the raters other than what would be expected by chance. 