\chapter{Proposed approaches}
\section{Basic overview}
\label{basover}
We want to deal with the problem of Out-of-Vocabulary (OOV) words in Text-to-speech and potentionally the ASR systems.
Our goal is to improve the phase, where the pronunciation in form of phonetic transcription is derived.
That means, we need to derive phonetic transcriptions of arbitrary words, correct pronunciation of which is unknown to us.
The straightforward way of achieving this is to use a \textit{grapheme-to-phoneme} ($g2p$) converter.
As we discuss in sections \ref{g2p-desc} and \ref{relatedwork}, this approach has some limitations, mainly because the pronunciation of some tokens in different languages may vary.
This means, that $g2p$ may provide pronunciation which would be correct if the world came from certain language $l$, but since it is not true, the pronunciation is actually wrong.
However, \textit{grapheme-to-phoneme conversion} is a well explored area and its output may serve as a sufficiently good baseline.
\par
In section \ref{relatedwork} we also discuss methods that are able to derive phonetic transcriptions directly from the speech signal, using a speech recognition framework. These methods are somewhat complex and they are typically specific for a particular recognizer. Rather than improving any of these methods, we aim to explore a possibility to combine them. That is, we want to exploit both textual and acoustic information to derive phonetic transcriptions of the desired words.
\par
\label{detection}
The question is, how to get audio samples containing the correctly pronounced spoken versions.
One option is to make the best effort when pronouncing the words.
In case that we pronounce some of the words incorrectly, we rely on the user's reaction, i.e. we expect he informs us about the mistake and subsequently provide a correct example.
Other option we have is to detect possible faults apriori and ask the user for the correct pronunciation directly.
Whole process may look like this:
\begin{enumerate}
\item Lookup the desired word in the pronunciation lexicon. In case it is not found here proceed to the classification step:
\item Decide if the word is difficult to pronounce for our Text-to-speech system.
\item If it is marked as difficult, ask the user for correct pronunciation.
\end{enumerate}
In the desired settings, the acoustic data with user's recordings are obtained online.
We mean by this a situation, when the system asks user for a correct pronunciation directly.
It can be difficult to make user say words we are interested in, so the ultimate goal is to develop an algorithm which is able to extract those words from a dialogue history or ask user to say them in a not irritating way.
The Crucial point of this approach is, that the user should not feel bored by the process. For the purpose of this work, we assume that we have already obtained recordings with correct pronunciations and work with it.
In the real setting we would have to employ a dialogue policy to be able to gather our own recordings or process the history and try to identify respective words.
\section{Used techniques}
\subsection{Detecting difficult words}
\label{ident-diff}
If we want to improve pronunciation of the synthesized recordings, we should first identify words that are mispronounced. Obviously, we could let the TTS system pronounce each word in a best effort style and  the user would identify mistakes and correct them. This approach has several drawbacks. First, the communication with the user is rather complicated, since the incorrectly pronounced word has to be isolated prior to obtaining the correct pronunciation.
Second, it may be unpleasant for the user if he or she has to undergo this process and hear the incorrect pronunciation. Thus, it would be better if we could recognize the possibly difficult words somehow. It would mean that we can ask the user directly for the correct pronunciation of the word.
\subsection{Measuring the difficulty}
In order to estimate the difficulty of each word's pronunciation, we propose three measures in this section. The key idea is that we obtain values for each of this measures and then combine them in one feature vector that represents the recording.
Then we can train a classifier\ that learns to predict quality of the pronunciation.
\subsubsection{$M_1$ measure - averaged Mel Cepstral Distortion}
\label{m1}
We discuss Mel Cepstral Distortion in detail in \ref{MCD-desc}. We use it here to define the $M_1$ measure.
We use synthesizers described in section \ref{usedtts} to synthesize the words. Thus we obtain four recordings per word. 
We can describe it as follows:
\begin{equation}
M_1(k) = \frac{1}{N}\sum_{(i,j)}{MCD(r_{ki},r_{kj})}
\end{equation}
Where $N = {4\choose2}$, $(i,j)$ stands for every combination of synthesizers, $r_{ki}$ is word {k} synthesized by engine {i} and $MCD$ is the Mel Cepstral Distortion.
The key idea motivating this measure is that if there is a problem with a pronunciation of some part of the word, every synthesizer has to deal with it somehow. It is likely, that each of them will do it in sligthly different way. This implies that the pairwise $MCD$ will increase.
\subsubsection{$M_2$ measure - averaged phonetic distance}
\label{m2}
Another measure we can possibly use is based on phonetic transcriptions. We first recognize the recordings with a phoneme recognizer, thus we obtain a sequence of characters per each recording. We can compute pairwise distances of these transcriptions, using Levenshtein \cite{navarro2001guided} distance which we normalize by the length of the word. The motivation is similar to the $M_1$ measure. Assuming the difficult words have positions that are problematic for the TTS engine, the recognized phonemes on these positions should differ and thus the distance between transcriptions should increase. Four our purposes, we have used Levenshtein distance as a metric.
\par
The M2 measure is described by equation:
\begin{equation}
M_2(k) = \frac{1}{N}\sum_{(i,j)}{\frac{LD(Hyp(r_{ki}),Hyp(r_{kj}))}{max(len(r_{ki},len(r_{kj})))}}
\end{equation}
Where $N = {4\choose2}$, $(i,j)$ stands for every combination of synthesizers and $r_{ki}$ is name $\{k\}$ synthesized by engine $\{i\}$. $LD$ means Levenshtein Distance and $Hyp$ represents the best hypothesis from the phonetic recognizer. Note, that we normalize the distance by length of the longer transcription.
\subsubsection{$M_3$ measure - occurrence of confusable bigrams}
\label{m3}
The $M_3$ measure is based on a different approach.
We want to learn the typical mistakes of a grapheme-to-phoneme converter, more specificaly we want to explore which groups of graphemes are difficult to transcribe for the $g2p$.
We suppose that words with many occurences of such groups are difficult to pronounce.
To compute the measure, we need two corpuses ($C_1$ and $C_2$) in different languages and a $grapheme-to-phoneme\:(g2p)$ training algorithm.
The corpuses should consist of pairs (word, transcription).
We can then prepare for computation of $M_3$ in three stages:
\begin{enumerate}
\item Train $g2p$ model $G_1$ on corpus $C_1$ in language $L_1$.
\item Use trained $G_1$ to transcribe words contained in corpus $C_2$.
\item Identify problematic parts.
\end{enumerate}
Stage 3 needs further description.
\par
We obtained list of triples, $i^{th}$ of which is structured:
\begin{center}
(word $w_i$ from $C_2$, original transcription $t^o_i$, transcription $t^h_i$ derived by $G_1$)
\end{center}
We can then use the Dynamic Time Warping algorithm to obtain pairwise alignments of these sequences, illustrated on \figref{sample_dtw_alignment}
In fact, we first transform the phoneme sequences into sequences of bigrams.
This is because graphemes and phonemes are not in one-to-one correspondence and bigrams capture the relations better.
\img{sample_align.jpg}{0.35}{Sample alignment of two phonetic sequences}{sample_dtw_alignment}
Once we have the alignments, we can identify positions, in which the original transcriptions from $C_2$ differ from the hypothesis derived in Stage 2.
Respective bigrams from the original word can then be identified.
For each bigram, we count number of times it was marked as difficult.
Each word can then be scored according to number of bad bigrams it contains.
\subsection{Phoneme recognition}
\label{ph-rec}
To improve the pronunciation, it is essential to process the user's recording and be able to obtain essential information from it.
We need to work on the phonetic level, so we have to get phonetic transcription of the recording.
We tried different methods of decoder's modification to output phonemes.
They vary in difficulty and quality so we explored the behavior.
\par
In theory, we could use the standard output of the speech recognizer, i.e. a sequence of words and transcribe it phonetically.
This has two essential problems:
\begin{enumerate}
\item Such procedure adds new source of potential errors, because we would have to use some mechanism, $g2p$ module for instance, that would create the transcription. This mechanism is error prone and we do not have a reliable one - if we had, we would not have to exploit the recording in the first place.
\item Furthermore, since we aim to add new words to the TTS vocabulary, it is likely, that the word is missing also in the vocabulary of the speech recognition system.
This fact means that the desired word will not appear in the recognizer's output, different word will appear instead.
Thus we cannot obtain any relevant transcription.
\end{enumerate}
Instead of this, we can modify the decoder used in the speech recognizer in such way, that it outputs sequences of phonemes rather than words.
This can be done in several ways, some of which we introduce in the following text.
\subsubsection{On the Automatic Speech Recognition (ASR) decoding process}
\label{asr-decoding}
We use the Kaldi framework\footnote{http://kaldi-asr.org/} for the ASR task and follow the procedure suggested in the official documentation.
Kaldi constructs a Finite State Transducer (FST) for purposes of the decoding process\footnote{http://kaldi-asr.org/doc/graph.html}.
This transducer is created as a composition of four other FSTs, $HCLG = H \circ C \circ L \circ G$.
The meanings of the parts are as follows:
\begin{itemize}
\item $G$ is an acceptor (i.e. its input and output symbols are the same) that encodes the grammar or language model.
\item $L$ is the lexicon; its output symbols are words and its input symbols are phones.
\item $C$ represents the context-dependency: its output symbols are phones and its input symbols represent context-dependent phones, i.e. windows of $N$ phones
\item $H$ contains the acoustic model's definitions; its output symbols represent context-dependent phones and its input symbols reference the model.
\end{itemize}
As we can see, the $G$ FST brings information contained in the language model to the decoding graph.
We provide an example in \figref{Lfst} which shows, that the $L$ transducer determines the output elements.
Hence, by modification of $G$ and $L$ we can change the output set of the decoded tokens.
\img{Lfst.png}{0.9}{An example of the finite state transducer representing the lexicon\cite{mohri2002weighted}. Labels "a:b" represents the edge's input and output symbols. This transducer is not weighted, since it represents only the lexicon, i.e. all possibilities. The decoding process goes from left to right, transforming the matching input string to the output.}{Lfst}
\subsubsection{Modifying the Language Model}
Language Model (LM) is an essential part of every ASR engine.
It provides probabilities that a particular word occur in the considered context. 
For example we can imagine a model that assigns probability to every possible sequence of words.
The length of the considered sequence can be restricted to $n$.
Such sequences are usually referred to as \textit{n-grams}.
The easiest approach is to use a so called 0-gram LM.
It simply distributes the probability mass uniformly among the words, i.e. each word occurs with same probability, independently of the context.
\par
Obviously, this model is not very good.
If we want to use more sophisticated approach, we can use collected textual data and estimate probabilities of word $n$-grams.
Since we consider phonemes, we can construct a Language Model that describes probabilities of phoneme occurrences instead of words.
\par
We can then modify $L$ and $G$ transducers to change the output of the recognizer to phonemes.
Ideally, we would use a set of name transcriptions to train the language model because it enrich the model with the domain information.
However, it is difficult to get corpus that consists of phonetic transcriptions and yet is large enough to robustly estimate the LM parameters.
So we create an artificial training dataset by taking an ordinary corpus of words and transcribing with a $g2p$ module.
\par
Alternatively, we could use phonetic alignments that are created during the decoding process and can be extracted from the recognizer.
The phonetic alignments are derived during the decoding, it holds information, which phonemes were used.
However, the Automatic Speech Recognition decoder in its common form restricts the output to contain only words that are present in its vocabulary.
The phoneme sequences corresponds to this words and therefore no unknown word (phoneme sequence) can appear in this alignments.
In other words, that transcriptions are influenced not only by the acoustic information but also by the Language Model.
Therefore, this approach is not applicable for us.
\par
The decoding graph is created using vocabulary and phonetic dictionary.
The vocabulary contains a list of known words and the dictionary respective phonetic transcriptions.
To change the transducers in the above (\ref{asr-decoding}) mentioned way, we reduce the vocabulary to list of phonemes and setting the dictionary mapping to be identity.
This way we force the decoder to output phonemes.
\subsubsection{Timit dataset}
The previous approach has a disadvantage that it is difficult to obtain a Language Model of good quality without a proper corpus of transcriptions.
Such corpus may be found as a part of the Timit dataset \cite{lopes2011phoneme}.
In fact, the Kaldi framework contains scripts, that train a model using this dataset.
The Timit dataset has got transcriptions labeled with granularity of individual phonemes.
This means we can train the phonetic recognizer directly.
Problem is that different set of phonemes is used but we can solve it by mapping the phonemes to IPA. 
\subsubsection{Modeling phoneme bigrams}
The decoding graph is context dependent. 
It means that the decoder takes into account both the right and the left context of the phonemes. 
In order to model the pronunciation on various positions in the word, each phoneme is represented in several variants, depending in which part of the word it is located (\textit{beginning, end, inside}). 
One option is to model each phoneme as a singleton, that is context-independent unit, as we have done in previous.
Alternatively, we can create a decoding graph, that models bigrams of phonemes.
This way the information about the context is preserved.
Moreover, we can model the input more accurately and distinguish pauses between words from spaces between phonemes.
\paragraph{evaluation}
We experimented with the mentioned setups (changing the LM, training the recognizer directly on phonemes, modelling phoneme bigrams).
The latter approach proved to yield the best results (\ref{recoexp}).
\subsubsection{Evaluation of phoneme recognizers}
\label{p-eval}
In ASR systems, common measure is the Word Error Rate.
It measures the ratio of incorrectly recognized words.
Since we are interested in phoneme recognition, we use Phone Error Rate (PER), which is basically WER when considering phonemes as words.
First, we define Phone Accuracy Rate:
\begin{equation}
Accuracy = \frac{N_T - S - D - I}{N_T} \times 100\%
\end{equation}
where $N_T$ is the total number of phonemes in the gold utterance and $S, D$ and $I$ are the substitution, deletion and insertion errors, respectively.
From this perspective, we can see it as related to Levenshtein distance of the refrence and test sequences.
Then, we can define $PER \: = \: 100\% - Accuracy$.
Note, that PER could be greater than 100\%, if the decoded string is longer than the input.
\par
Another measure is correctness, which is similar to accuracy, but does not consider insertion errors.
\par
The number of insertion, deletion and substitution errors is computed using the best alignment between two token sequences: the manually aligned (reference) and the recognized (test).
An alignment resulting from search strategies based on dynamic programming is normally used successfully for a large number of speech recognition tasks \cite{ney2000progress}.
We use a $sclite$ utility\footnote{http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm} to measure PER.
\subsubsection{Exploring the speech recognizer}
\label{nbest-detail}
The decoding process in Automatic Speech Recognition yields a list of hypotheses, ordered by their confidence scores, so called $n$-best list.
Usually, the best hypothesis is chosen and used since it is the most relevant result.
However, interesting information may be found deeper in the list.
We explore this assumption in the following paragraphs.
\par
In section \ref{p-eval} we have introduced Phone Error Rate (PER).
We extend the term here and define \textbf{Oracle Phone Error Rate}:
\begin{equation}
OPER = \minop_{h \in hypotheses} PER(h, gold)
\end{equation}
We can see that Oracle Phone Error Rate is PER of the best hypothesis (if we consider the hypothesis with the lowest PER according to refrence as the best one.)
In real setup we obviously does not have such oracle that would tell us which hypothesis is actually the best
So if we want to evaluate it, we have no other option but look at the top of the $n$-best list, compute PER for each hypothesis and choose the one that is the closest to reference transcription.
Hence we approximate the true Oracle PER, since if we explore "sufficiently many" best alternatives, the probability that we did not see the best one is very low.
We give an overview of our results in \figref{OPER}.
\img{OPER.png}{0.9}{A plot of Oracle Phone Error Rate dependency on the depth of the $n$-best list we explore. We can see, that	 the Oracle PER decreases with the depth, so it makes sense to try to exploit the hypotheses located deeper.}{OPER}
\par
Using Oracle Phone Error Rate to evaluate the recognizer is basically cheating, since in real decoding process, we are not given the gold transcription which we could use to pick the best hypothesis.
Nevertheless, it indicates some properties of the decoder, namely that the hypothesis located at the top is not always the best one.
We explore this phenomenon further by looking at the particular position of the closest (best) hypothesis.
In \figref{besthyp} we plot the histogram showing, number of times the closest hypothesis appeared in particular depth.
We can see, that relevant hypotheses can be found in bigger depth.
\img{besthypdistr.png}{0.9}{Histogram that visualizes the distribution of the hypothesis, that is the closest to the gold reference utterance in terms of Phone Error Rate. The bars show the number of times the best hypothesis was found on the particular position. Two settings are included in this figure: the green bars describe situation, when first five positions were used, the blue ones were measured using nine positions.}{besthyp}
\par
The observations we have made in the previous text indicates that there is a substantial piece of information contained in the phoneme recognizer's $n$-best list.
The problem of the $n$-best list is, that many pairs of hypothesis differs only in a few positions.
That is because of the nature of the procedures used for the decoding.
It is quite intuitive, that hypothesis that differ only in one position have similar probabilities.
We would like to use the information but we are not interested in many hypotheses that are nearly the same.
So we propose to clusterize the hypotheses contained in the $n$-best list.
The goal is to obtain a more compact version of the list with high variability among items.
We present the results in section \ref{nbest-clust}, where we also compare it with the baseline.